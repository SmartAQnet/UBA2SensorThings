{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialisations\n",
    "## 1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import sys\n",
    "import time\n",
    "import requests, json\n",
    "import numpy as np\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import random\n",
    "import glob\n",
    "import hashlib\n",
    "#import easygui\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Read Metadata Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destinationurl = \"http://smartaqnet-dev.dmz.teco.edu/v1.0\"\n",
    "destinationurl = \"http://api.smartaq.net/v1.0\"\n",
    "\n",
    "file = pd.read_excel('metadata/Bericht_EU_Meta_Stationen.xlsx')\n",
    "filemeta=pd.read_excel('metadata/Bericht_EU_Meta_Stationsparameter.xlsx')\n",
    "df_stationparameters = filemeta.set_index(\"station_code\")\n",
    "\n",
    "#list of features with their component codes as appears in 'metadata/Bericht_EU_Meta_Stationsparameter.xlsx'\n",
    "listofreplacements=[\n",
    "    ('PM10','5.0','mcpm10'),\n",
    "    ('PM2,5','6001.0','mcpm2p5'),\n",
    "    ('PM1','6002.0','mcpm1'),\n",
    "]\n",
    "\n",
    "#federal state, network code (as appears in 'metadata/Bericht_EU_Meta_Stationen.xlsx'), operator-url\n",
    "ccodesetreadable=[('Hessen', 'DE009A','hlnug.de'),\n",
    " ('Saarland', 'DE001A','saarland.de'),\n",
    " ('Berlin', 'DE008A','berlin.de'),\n",
    " ('Bayern', 'DE007A','lfu.bayern.de'),\n",
    " ('Rheinland-Pfalz', 'DE011A','luft.rlp.de'),\n",
    " ('Sachsen', 'DE016A','umwelt.sachsen.de'),\n",
    " ('Umweltbundesamt', 'DE006A','umweltbundesamt.de'),\n",
    " ('Baden-Wuerttemberg', 'DE005A','lubw.baden-wuerttemberg.de'),\n",
    " ('Nordrhein-Westfalen', 'DE004A','lanuv.nrw.de'),\n",
    " ('Brandenburg', 'DE014A','lfu.brandenburg.de'),\n",
    " ('Bremen', 'DE013A','bauumwelt.bremen.de'),\n",
    " ('Mecklenburg-Vorpommern', 'DE018A','lung.mv-regierung.de'),\n",
    " ('Hamburg', 'DE012A','luft.hamburg.de'),\n",
    " ('Thueringen', 'DE017A','tlug-jena.de'),\n",
    " ('Schleswig-Holstein', 'DE002A','schleswig-holstein.de'),\n",
    " ('Sachsen-Anhalt', 'DE015A','luesa.sachsen-anhalt.de'),\n",
    " ('Niedersachsen', 'DE010A','umwelt.niedersachsen.de')]\n",
    "\n",
    "\n",
    "#returns the url for the network code\n",
    "def repnetcodebyurl(netcode):\n",
    "    for item in ccodesetreadable:\n",
    "        if item[1]==netcode:\n",
    "            return(item[2])\n",
    "\n",
    "#returns the name of the state for the network code\n",
    "def repnetcodebystate(netcode):\n",
    "    for item in ccodesetreadable:\n",
    "        if item[1]==netcode:\n",
    "            return(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. General Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts numpy types to python types, otherwise json conversion produces an error. call json.dumps(***, cls=MyEncoder)\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "        \n",
    "\n",
    "# returns strings in the conventional format for iot.ids\n",
    "def idstr(idinput):\n",
    "    return(str(idinput).lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"ä\", \"ae\").replace(\"ö\", \"oe\").replace(\"ü\", \"ue\").replace(\"ß\", \"ss\"))\n",
    "\n",
    "\n",
    "# returns the first 7 digits of the sha1 hash of the input string\n",
    "def hashfunc(inputstring, printme):\n",
    "    returnhash= hashlib.sha1(bytes(str(inputstring), 'utf-8')).hexdigest()[0:7]\n",
    "    if printme == True:\n",
    "        print(\"Converting '\" + str(inputstring) + \"' to hash '\" + str(returnhash) +\"'\")\n",
    "    return(returnhash)\n",
    "\n",
    "\n",
    "\n",
    "# returns the full 40 digits of the sha1 hash of the input string\n",
    "def hashfuncfull(inputstring, printme):\n",
    "    returnhash= hashlib.sha1(bytes(str(inputstring), 'utf-8')).hexdigest()\n",
    "    if printme == True:\n",
    "        print(\"Converting '\" + str(inputstring) + \"' to hash '\" + str(returnhash) +\"'\")\n",
    "    return(returnhash)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#functions for time conversion\n",
    "def readtime(sec):\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return(str(int(h)) + \" hours \" + str(int(m)) + \" minutes \" + str(int(s)) + \" seconds\")\n",
    "\n",
    "def tounixtime(datetime_input):\n",
    "    return(calendar.timegm(datetime_input.utctimetuple()))\n",
    "\n",
    "def todatetimeformat(utctime):\n",
    "    year=int(utctime[0])*1000 + int(utctime[1])*100 + int(utctime[2])*10 + int(utctime[3])\n",
    "    month=int(utctime[5])*10 + int(utctime[6])\n",
    "    day=int(utctime[8])*10 + int(utctime[9])\n",
    "    hr=int(utctime[11])*10 + int(utctime[12])\n",
    "    minute=int(utctime[14])*10 + int(utctime[15])\n",
    "    second=int(utctime[17])*10 + int(utctime[18])\n",
    "    millisecond=int(utctime[20])*100 + int(utctime[21])*10 + int(utctime[22])   \n",
    "    return(datetime(year,month,day,hr,minute,second,millisecond))\n",
    "\n",
    "def toutcformat(datetime_input):\n",
    "    tstr=str(datetime_input)\n",
    "    year=tstr[0]+tstr[1]+tstr[2]+tstr[3]\n",
    "    month=tstr[5]+tstr[6]\n",
    "    day=tstr[8]+tstr[9]\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[11]+tstr[12]))==int:\n",
    "            hour=str(tstr[11]+tstr[12])\n",
    "    except:\n",
    "        hour='00'             #no hours given       \n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[14]+tstr[15]))==int:\n",
    "            minute=str(tstr[14]+tstr[15])\n",
    "    except:\n",
    "        minute='00'             #no minutes given\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[17]+tstr[18]))==int:\n",
    "            second=str(tstr[17]+tstr[18])\n",
    "    except:\n",
    "        second='00'             #no seconds given\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[20]+tstr[21]+tstr[22]))==int:\n",
    "            millisecond=str(tstr[20]+tstr[21]+tstr[22])\n",
    "    except:\n",
    "        millisecond='000'       #no milliseconds given\n",
    "\n",
    "\n",
    "\n",
    "    utctime=year + '-' + month + '-' + day + 'T' + hour + ':' + minute + ':' + second + '.' + millisecond + 'Z'\n",
    "    return utctime\n",
    "\n",
    "\n",
    "def addminutesutc(utctime,mins):\n",
    "    return(toutcformat(datetime.utcfromtimestamp(tounixtime(todatetimeformat(utctime))+(mins*60))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function generatemetadata() \n",
    "## - models the Database from the Metadata Files\n",
    "## - gets the Observations from UBA url and saves them into an excel file\n",
    "## - If upload set to true will execute parseexcel() to upload the Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#function that builds the metadata\n",
    "\n",
    "\n",
    "def generatemetadata():\n",
    "\n",
    "    #input values. Read config.txt\n",
    "\n",
    "    try:\n",
    "        with open('config.txt') as configfile:\n",
    "            config=json.loads(configfile.read())\n",
    "\n",
    "            url = config['url']\n",
    "            thingcode = config['thingcode']\n",
    "            feature = config['feature']\n",
    "        #    intervalllength = config['intervalllength']\n",
    "            if config[\"runparseaftermodelling\"] == True:\n",
    "                upload == True #set to True/False to enable/disable upload of metadata to FROST\n",
    "    except:\n",
    "        sys.exit(\"Config File not properly set!\")\n",
    "\n",
    "    baseurl = 'https://www.umweltbundesamt.de/js/uaq/data/stations' #get data where from\n",
    "    scope='1SMW' #umweltbundesamt website code: 1 hour means\n",
    "    scopesec= 60*60 #scope in seconds, needed for interval to tag the next observation\n",
    "\n",
    "    print(\"Upload is set to \" + str(upload))\n",
    "    print(\"Extracting Observations for ObservedProperty \" + str(feature))\n",
    "\n",
    "    #fetches the code for the 'feature' defined in listofreplacements\n",
    "    for eachelement in listofreplacements:\n",
    "        if eachelement[0] == feature:\n",
    "            code = eachelement[1]\n",
    "            op_id_variable = eachelement[2]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print for human crosscheck\n",
    "    print('Check: Going to uploading data for station ' + str(thingcode) + ' at ' + str(file.set_index(\"station_code\")[\"station_name\"][thingcode]))\n",
    "\n",
    "    #metadata\n",
    "    totalstarttime = time.time() #to check how long the upload took\n",
    "    \n",
    "    #observed property --> should already be \"officially\" created!\n",
    "    op_tohash = idstr(op_id_variable)\n",
    "#    generatepropertyid = \"saqn:op:\" + hashfunc(op_tohash, True)\n",
    "    generatepropertyid = \"saqn:op:\" + op_tohash\n",
    "\n",
    "    if requests.head(url +  \"/ObservedProperties\" + \"('\" + generatepropertyid + \"')\").status_code == 200:\n",
    "        print(generatepropertyid + \" exists.\")\n",
    "    else:\n",
    "#        print(generatepropertyid + \" does NOT exist. Will generate dummy that needs to be patched. Upload is \" + str(upload))\n",
    "#        obsproperty = {\n",
    "#            \"name\": str(feature),\n",
    "#            \"description\": str(feature),\n",
    "#            \"definition\": \"\",\n",
    "#            \"@iot.id\": generatepropertyid\n",
    "#            }\n",
    "#        if upload==True:\n",
    "#            requests.post(url + '/ObservedProperties', json.dumps(obsproperty))\n",
    "#        else:\n",
    "#            pass\n",
    "        sys.exit(\"Observed Property \" + str(feature) + \" with id \" + generatepropertyid + \" does not exist! Abort.\")\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    thingnr=list(file[\"station_code\"]).index(thingcode) #the number of the row in the excel file\n",
    "\n",
    "    generatedescr=\"\" #generates the description for the thing\n",
    "    if  df_stationparameters[\"type_of_parameter\"].index.contains(thingcode): #checks whether the station actually exists\n",
    "        if thingcode in df_stationparameters[\"type_of_parameter\"][thingcode]: #if the stations measures only one type of parameter, the index is not returned somehow and the loop produces an error, therefore check if the index is returned and if not handle case separately\n",
    "            for element in list(set(df_stationparameters[\"type_of_parameter\"][thingcode])):\n",
    "                generatedescr+= \" -\" + element + \"-\"\n",
    "        else: #if the station only measures one type of parameter (loop produces an error in that case, thus handled separately)\n",
    "            generatedescr+= \" -\" + df_stationparameters[\"type_of_parameter\"][thingcode] + \"-\"\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    #building things\n",
    "\n",
    "    #Location ID\n",
    "    loc_id_prefix = \"geo:\" \n",
    "    loc_id_lat = str(float(file[\"station_latitude_d\"][thingnr]))\n",
    "    loc_id_lon = str(float(file[\"station_longitude_d\"][thingnr]))\n",
    "    loc_id_alt = str(float(file[\"station_altitude\"][thingnr]))\n",
    "    \n",
    "    loc_tohash = idstr(loc_id_lat + \",\" + loc_id_lon + \",\" + loc_id_alt)\n",
    "    \n",
    "    generatelocid = loc_id_prefix + loc_tohash #hashfunc(loc_tohash, True)\n",
    "\n",
    "    #Thing ID\n",
    "    thing_id_prefix = \"saqn:t:\"\n",
    "    thing_id_url = str(repnetcodebyurl(file[\"network_code\"][thingnr]))\n",
    "    thing_id_thingname = \"Station \" + str(file[\"station_name\"][thingnr])\n",
    "    thing_id_date = str(file[\"station_start_date\"][thingnr])[0:4] + \"-\" + str(file[\"station_start_date\"][thingnr])[4:6]\n",
    "    thing_id_thingnumber = str(file[\"station_code\"][thingnr])\n",
    "    \n",
    "    thing_tohash = idstr(thing_id_url + \":\" + thing_id_thingname + \":\" + thing_id_date + \":\" + thing_id_thingnumber)\n",
    "    \n",
    "    generatethingid = thing_id_prefix + hashfunc(thing_tohash, True)\n",
    "\n",
    "    #generates a dictionary of all raw properties of the thing\n",
    "    rawproperties = {}\n",
    "    for eachproperty in list(file):\n",
    "        if str(file[eachproperty][thingnr])=='nan':\n",
    "            rawproperties[eachproperty]='nan'\n",
    "        else:\n",
    "            rawproperties[eachproperty] = file[eachproperty][thingnr]\n",
    "    rawproperties[\"operator_url\"] = thing_id_url\n",
    "\n",
    "    #generate the thing JSON\n",
    "    thingdata = {\"name\": thing_id_thingname,\n",
    "        \"description\": \"A station measuring\" + str(generatedescr),\n",
    "        \"properties\": rawproperties,\n",
    "        \"@iot.id\": idstr(generatethingid),\n",
    "         \"Locations\": [{\n",
    "            \"name\": \"Location at latitude \" + loc_id_lat + \" and longitude \" + loc_id_lon,\n",
    "            \"description\": \"located at \" + str(file[\"station_name\"][thingnr]),\n",
    "            \"encodingType\": \"application/vnd.geo+json\",\n",
    "            \"@iot.id\": idstr(generatelocid),\n",
    "            \"location\": {\n",
    "                  \"type\": \"Point\",\n",
    "                  \"coordinates\": [float(file[\"station_longitude_d\"][thingnr]), float(file[\"station_latitude_d\"][thingnr]), float(file[\"station_altitude\"][thingnr])]\n",
    "            }\n",
    "\n",
    "          }]\n",
    "    }\n",
    "    if upload==True:\n",
    "        requests.post(url + '/Things', json.dumps(thingdata, cls=MyEncoder))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    #loop over all sensors and check which contains the requested observedproperty\n",
    "    try:\n",
    "        if float(code) not in list(df_stationparameters[\"component_code\"][thingcode]):\n",
    "            print(\"Station \" + str(thingcode) + \" does not measure \" + str(feature))\n",
    "        else:\n",
    "            for sensnr in range(len(list(df_stationparameters[\"component_code\"][thingcode]))):\n",
    "                if list(df_stationparameters[\"component_code\"][thingcode])[sensnr] == float(code):\n",
    "                    thissensor=list(df_stationparameters[\"parameter\"][thingcode])[sensnr] #the parameter to parse, e.g. \"Particulate Matter - PM10, first measurement\"\n",
    "                    mestech=list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]\n",
    "                    #warning: if only one sensor exists, this will blow up because one item is not returned as dataframe but as string\n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------\n",
    "                    #building the sensors\n",
    "\n",
    "\n",
    "                    #Sensor ID\n",
    "                    sensor_id_prefix = \"saqn:s:\"\n",
    "                    sensor_id_url = str(repnetcodebyurl(file[\"network_code\"][thingnr]))\n",
    "                    sensor_id_name = \"generic \" + str(list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]) + \" sensor\"\n",
    "                    \n",
    "                    sensor_tohash = idstr(sensor_id_url + \":\" + sensor_id_name)\n",
    "                    \n",
    "                    generatesensorid = sensor_id_prefix + hashfunc(sensor_tohash, True)\n",
    "\n",
    "\n",
    "                    #generate sensor JSON\n",
    "                    sensor = {\"name\": sensor_id_name,\n",
    "                            \"description\": \"A sensor measuring \" + str(feature) + \" using \" + str(mestech),\n",
    "                            \"properties\": {\"operator_url\": sensor_id_url},\n",
    "                            \"encodingType\": \"\",\n",
    "                            \"metadata\": \"\",\n",
    "                            \"@iot.id\": idstr(generatesensorid)\n",
    "                            }\n",
    "                    if upload==True:\n",
    "                        requests.post(url + '/Sensors', json.dumps(sensor, cls=MyEncoder))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------\n",
    "                    #building the datastreams\n",
    "                    \n",
    "                    #Datastream ID\n",
    "                    stream_id_prefix = \"saqn:ds:\"\n",
    "                    stream_id_url = str(repnetcodebyurl(file[\"network_code\"][thingnr]))\n",
    "                    stream_id_sensorname = sensor_id_name\n",
    "                    stream_id_sensornumber = \"\"\n",
    "                    \n",
    "                    stream_tohash = idstr(stream_id_url + \":\" + stream_id_sensorname + \":\" + stream_id_sensornumber)\n",
    "                    fullstream_tohash = thing_tohash + \":\" + stream_tohash + \":\" + op_tohash\n",
    "                    \n",
    "                    generatestreamid = stream_id_prefix + hashfunc(fullstream_tohash, True)\n",
    "     \n",
    "                    #generates a dictionary of all raw properties of the thing to dump into metadata property\n",
    "                    uploadmetadata = {}\n",
    "#                    uploadmetadata[\"script version number\"]=\"1.0\"\n",
    "#                    uploadmetadata[\"uploaded data from\"]=configuration[\"starttime\"]\n",
    "#                    uploadmetadata[\"uploaded data until\"]=configuration[\"endtime\"]\n",
    "                    \n",
    "                    rawmetadata = {}\n",
    "#                    rawmetadata[\"station_code\"]=thingcode\n",
    "                    rawmetadata[\"operator_url\"]=stream_id_url\n",
    "                    rawmetadata[\"upload properties\"]=uploadmetadata\n",
    "        \n",
    "                    license = {}\n",
    "                    license[\"type\"] = \"\"\n",
    "                    license[\"owner\"] = \"Umweltbundesamt\"\n",
    "                    \n",
    "                    rawmetadata[\"license\"]=license\n",
    "                    \n",
    "                    for eachdata in list(df_stationparameters): #option 1: all metadata\n",
    "                    #for eachdata in [\"type_of_parameter\",\"parameter\",\"component_code\",\"measurement_technique_principle\"]: #option 2: pick\n",
    "                        if str(list(df_stationparameters[eachdata][thingcode])[sensnr]) == 'nan':\n",
    "                            rawmetadata[eachdata]='nan'\n",
    "                        else:\n",
    "                            rawmetadata[eachdata] = list(df_stationparameters[eachdata][thingcode])[sensnr]\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                    print(\"Building Datastream \" + idstr(generatestreamid))\n",
    "\n",
    "                    datastream = {\"name\": str(thissensor) + \" Datastream of station \" + str(thingcode),\n",
    "                                \"description\": \"A Datastream measuring \" + str(thissensor) + \" using \" + str(mestech),\n",
    "                                \"observationType\": \"http://www.opengis.net/def/observationType/OGC-OM/2.0/OM_Measurement\",\n",
    "                                \"unitOfMeasurement\": {\n",
    "                                    \"name\": \"microgram per cubic meter\",\n",
    "                                    \"symbol\": \"ug/m^3\",\n",
    "                                    \"definition\": \"http://www.qudt.org/qudt/owl/1.0.0/unit/Instances.html#KilogramPerCubicMeter\"\n",
    "                                    },\n",
    "                                \"properties\": rawmetadata,\n",
    "                                \"@iot.id\": idstr(generatestreamid),\n",
    "                                \"Thing\":{\"@iot.id\":idstr(generatethingid)},\n",
    "                                \"Sensor\":{\"@iot.id\":idstr(generatesensorid)},\n",
    "                                \"ObservedProperty\":{\"@iot.id\":idstr(generatepropertyid)}\n",
    "                                }\n",
    "\n",
    "                    if upload==True:\n",
    "                        requests.post(url + \"/Things('\" + idstr(generatethingid) + \"')/Datastreams\", json.dumps(datastream))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    #END OF METADATA - - - BEGIN OF DATA UPLOAD\n",
    "                    #------------------------------------------------------------\n",
    "\n",
    "                    def graburl(start,end): #start and end in datetime\n",
    "                        return(baseurl + '/measuring?pollutant[]=' + feature + '&scope[]=' + scope + '&station[]=' + thingcode + '&group[]=pollutant&range[]=' + str(tounixtime(start)) + ',' + str(tounixtime(end)))\n",
    "\n",
    "                    #begin time\n",
    "                    try:\n",
    "                        begintime=todatetimeformat(config['starttime'])\n",
    "                    except:\n",
    "\n",
    "                        #get the start month of the respective datastream\n",
    "\n",
    "                        def getstarttime():\n",
    "                            sys.stdout.write(\"Searching Datastream Start Time\")\n",
    "                            for i in range(1970,2018+1): #ranges to 2018, the +1 is because how range counts\n",
    "                                try:\n",
    "                                    theurl=graburl(datetime(i,1,1,0,0,0),datetime(i,12,31,23,59,0))\n",
    "                                    datafromurl=json.loads(requests.get(theurl).content)[\"data\"][0]\n",
    "                                    if datafromurl[0][0]!='bananas': #anything but an error\n",
    "                                        for j in range(1,12+1):\n",
    "                                            try:\n",
    "                                                theurl2=graburl(datetime(i,j,1,0,0,0),datetime(i,j,calendar.monthrange(i,j)[1],23,59,0))\n",
    "                                                datafromurl2=json.loads(requests.get(theurl2).content)[\"data\"][0]\n",
    "                                                if datafromurl2[0][0]!='bananas':\n",
    "                                                    for k in range(1,calendar.monthrange(i,j)[1]+1):\n",
    "                                                        try:\n",
    "                                                            theurl3=graburl(datetime(i,j,k,0,0,0),datetime(i,j,k,23,59,0))\n",
    "                                                            datafromurl3=json.loads(requests.get(theurl3).content)[\"data\"][0]\n",
    "                                                            if datafromurl3[0][0]!='bananas':\n",
    "                                                                return(datetime(i,j,k,0,0,0))\n",
    "                                                                break\n",
    "                                                        except:\n",
    "                                                            pass\n",
    "                                            except:\n",
    "                                                pass\n",
    "                                except:\n",
    "                                    sys.stdout.write(\".\")\n",
    "                                    pass\n",
    "\n",
    "                        begintime=getstarttime()\n",
    "\n",
    "                        print(\"\")\n",
    "                        print(\"Parsing start time not properly set. Taking earliest date measurements appear: \" + str(begintime))\n",
    "\n",
    "                    #end time\n",
    "                    try:\n",
    "                        endtime=todatetimeformat(config['endtime'])\n",
    "                    except:\n",
    "                        print(\"Parsing end time not properly set. Taking \" + str(datetime.utcnow()))\n",
    "                        endtime = datetime.utcnow() \n",
    "\n",
    "\n",
    "                    #------------------------------------------------------------\n",
    "\n",
    "                    try:\n",
    "                        begintimeunix=tounixtime(begintime)\n",
    "                        endtimeunix=tounixtime(endtime)\n",
    "                    except:\n",
    "                        print(\"Error! Datatstream empty! Cannot find any data!\")\n",
    "                        return()\n",
    "\n",
    "                    print('Measurements of ' + str(thingcode) + ' ' +  str(thissensor) + ' are being parsed from ' + str(begintime))\n",
    "\n",
    "\n",
    "                    getdatafrom=baseurl + '/measuring?pollutant[]=' + feature + '&scope[]=' + scope + '&station[]=' + thingcode + '&group[]=pollutant&range[]=' + str(begintimeunix + (scopesec/2)) + ',' + str(endtimeunix  + (scopesec/2))\n",
    "                    datavalue=json.loads(requests.get(getdatafrom).content)[\"data\"][0]\n",
    "\n",
    "\n",
    "                    #convert list into a dataframe\n",
    "                    datalist=[]\n",
    "                    labels=['interval_start_time','interval_end_time',str(feature)]\n",
    "\n",
    "                    for i in range(len(datavalue)):\n",
    "                            datalist.append([toutcformat(datetime.utcfromtimestamp(begintimeunix + (scopesec*i))),toutcformat(datetime.utcfromtimestamp(begintimeunix + ((scopesec*i) + (scopesec-60)) )),datavalue[i][0]])\n",
    "\n",
    "                    dataframe = pd.DataFrame.from_records(datalist, columns=labels)\n",
    "                    dataframemeta = pd.DataFrame.from_records([[idstr(generatestreamid)],[fullstream_tohash]], columns=['datastreamID'])\n",
    "\n",
    "                    \n",
    "                    #if already a file exists for that datastream and feature move it into archive before creating new\n",
    "                    try: #get the first file matching thingcode and feature e.g. DEBY007_PM10_*.xlsx\n",
    "                        data_dir=str(os.getcwd()) + \"/data/\"\n",
    "                                                \n",
    "                        existing_file = glob.glob(data_dir + str(thingcode) + \"*\" + str(feature) + \"*.xlsx\")[0]\n",
    "                        print(\"File for that datastream and observedProperty already exists. \")\n",
    "                        existing_file_new = str(existing_file)[:-5] + \"---\" + str(toutcformat(datetime.utcnow()))[:-5].replace(\":\",\"-\") + \".xlsx\"\n",
    "                        print(\"Renaming file \" + existing_file + \" to \" + existing_file_new)\n",
    "                        os.rename(existing_file, data_dir +'/archive/' + str(existing_file_new)[len(data_dir):])\n",
    "                        print(\"Moving file into archive folder\")\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    #save dataframe to excel sheet\n",
    "                    filename= str(thingcode) + \"_\" + str(thissensor) + \"_\" + str(toutcformat(begintime)[0:10]) + \"_\" + str(toutcformat(endtime)[0:10]) + \".xlsx\"\n",
    "                    writer = ExcelWriter('data/' +  str(filename))\n",
    "                    dataframe.to_excel(writer,'Sheet1',index=False)\n",
    "                    dataframemeta.to_excel(writer,'id',index=False)\n",
    "                    writer.save()\n",
    "\n",
    "                    #crosschecking: checks a number of random points between the dataframe and the url\n",
    "\n",
    "                    #the UBA database gives the result for the hour that has PASSED, e.g. the value at 5 o'clock is the mean between 4 and 5 o'clock\n",
    "                    #the code writes a value for the interval between e.g. 3:00 and 3:59. for this interval, the UBA value at 3:30 is taken. \n",
    "                    #therefore the check also needs to add 30m to the url to check with the corresponding start time\n",
    "\n",
    "                    numberoftests=10\n",
    "                    testistrue=False\n",
    "\n",
    "\n",
    "                    for i in range(0,10):\n",
    "                        r = random.randint(1,len(list(dataframe[\"interval_start_time\"])))\n",
    "                        getdatapoint=graburl(todatetimeformat(addminutesutc(dataframe[\"interval_start_time\"][r],30)),todatetimeformat(addminutesutc(dataframe[\"interval_end_time\"][r],30)))\n",
    "                        checkdatavalue=json.loads(requests.get(getdatapoint).content)[\"data\"][0][0][0]\n",
    "                        if dataframe[feature][r] == checkdatavalue:\n",
    "                            testistrue=True\n",
    "                        else:\n",
    "                            print(str(dataframe[feature][r]) + \"!=\" + str(checkdatavalue))\n",
    "                            testistrue=False\n",
    "\n",
    "                    print(\"Crosscheck for \" + str(thingcode) + \" \" + str(thissensor) + \" with \" + str(numberoftests) + \" random datapoints gave result \" + str(testistrue))\n",
    "\n",
    "\n",
    "        endtime=time.time()\n",
    "        timeelapsed=endtime-totalstarttime\n",
    "\n",
    "        print(\"Time elapsed: \" + str(readtime(timeelapsed)))\n",
    "\n",
    "        if upload==True:\n",
    "            parseexcel()\n",
    "\n",
    "    except:\n",
    "        print(\"Could not retrieve any data for station \" + str(thingcode))\n",
    "    return()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Function parseexcel() parses the Observation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "#function parses the data file\n",
    "\n",
    "\n",
    "\n",
    "def parseexcel():\n",
    "    #input parameters. Read config.txt\n",
    "\n",
    "    try:\n",
    "        with open('config.txt') as configfile:\n",
    "            config=json.loads(configfile.read())\n",
    "\n",
    "            url = config['url']\n",
    "            thingcode = config['thingcode']\n",
    "            feature = config['feature']\n",
    "            intervalllength = config['intervalllength']\n",
    "    except:\n",
    "        sys.exit(\"Config File not properly set!\")\n",
    "\n",
    "\n",
    "    #read data and auxiliary functions\n",
    "\n",
    "    #get the component code for a feature as long as it is listed in the listofreplacements\n",
    "    def componentcode(feat):\n",
    "        for rep in listofreplacements:\n",
    "            if rep[0]==feature:\n",
    "                return(rep[1])\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "     \n",
    "            \n",
    "            \n",
    "    try: #get the first file matching thingcode and feature e.g. DEBY007_PM10_*.xlsx\n",
    "        filename = glob.glob(\"data/\" + str(thingcode) + \"*\" + str(feature) + \"*.xlsx\")[0]\n",
    "    except:\n",
    "        print(\"No file to parse found that matches the requirements\")\n",
    "        pass\n",
    "    \n",
    "#    filename_path = easygui.fileopenbox(msg=None, title=None, default='Data/' + str(thingcode) + '*' + str(feature) + '*.xlsx', filetypes=None, multiple=False)\n",
    "#    filename= filename_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    print(\"Loading file \" + str(filename) + \"...\")\n",
    "    datafile=pd.read_excel(str(filename),0)\n",
    "    datafilemeta=pd.read_excel(str(filename),'id')\n",
    "\n",
    "    #clear all nullresults (False) before the first measurement\n",
    "    nullresults=0\n",
    "    for i in range(len(list(datafile[\"interval_start_time\"]))):\n",
    "        if datafile[feature][i]==0:\n",
    "            nullresults+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #read file again but skip the nullresults\n",
    "    datafile=pd.read_excel(str(filename),skiprows=range(1,nullresults+1))\n",
    "\n",
    "    #for displaying the elapsed time\n",
    "    starttime=time.time()\n",
    "\n",
    "\n",
    "    #get datastream id\n",
    "    datastreamID=datafilemeta[\"datastreamID\"][0]\n",
    "    fullstream_tohash=datafilemeta[\"datastreamID\"][1]\n",
    "\n",
    "    print(\"Uploading Observations for Datastream iot.id: \" + datastreamID)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "    for i in range(len(list(datafile[\"interval_start_time\"]))): #eleganter ist hier feature zu nehmen aber ist egal\n",
    "\n",
    "        observation_id_prefix = \"saqn:o:\"\n",
    "        observation_interval = str(datafile[\"interval_start_time\"][i]) + \"/\" + str(intervalllength)\n",
    "\n",
    "        observation_tohash = idstr(fullstream_tohash + \":\" + observation_interval)\n",
    "\n",
    "        generateobsid = observation_id_prefix + hashfuncfull(observation_tohash, False)\n",
    "\n",
    "        observation = {\n",
    "        \"phenomenonTime\" : observation_interval, \n",
    "        \"result\" : float(datafile[str(feature)][i]),\n",
    "        \"Datastream\":{\"@iot.id\": str(datastreamID)},\n",
    "        \"@iot.id\": generateobsid\n",
    "        }\n",
    "\n",
    "        requests.post(url + \"/Datastreams('\" + datastreamID + \"')/Observations\", json.dumps(observation))\n",
    "\n",
    "        #estimating time remaining for parsing\n",
    "        timeelapsed=time.time()-starttime\n",
    "        timeelapsedread=readtime(((len(list(datafile[\"interval_start_time\"]))/(i+1))-1)*int(timeelapsed+1))\n",
    "        sys.stdout.write('Uploaded ' + str(i) + ' out of ' + str(len(list(datafile[\"interval_start_time\"]))) + ' Observations. Estimating ' + timeelapsedread + ' remaining \\r') \n",
    "\n",
    "\n",
    "    endtime=time.time()\n",
    "    timeelapsed=endtime-starttime\n",
    "    \n",
    "    sys.stdout.write('\\n' + '__________________________________________________________________________' + '\\n')\n",
    "    sys.stdout.write('Finshed! Successfully uploaded ' + str(len(list(datafile[\"interval_start_time\"]))) + ' Observations in ' + str(readtime(int(timeelapsed))) + ' seconds. \\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. User Input determines which stations to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter UBA Station to parse: Augsburg\n",
      "Will parse the following stations: \n",
      "['DEBY006', 'DEBY007', 'DEBY008', 'DEBY099', 'DEBY110', 'DEBY150']\n",
      "If you do not input a start time, the earliest date a measurement appears will be taken. \n",
      "Input start time (yes/no) : yes\n",
      "Enter start time year : 2019\n",
      "Enter start time month : 04\n",
      "Enter start time day : 01\n",
      "Enter start time hour : 00\n",
      "Enter start time minutes : 00\n",
      "If you do not input an end time, 'right now' will be taken. \n",
      "Input end time (yes/no) : \n",
      "Upload data to server after writing to file (yes/no): yes\n",
      "Parsing 6 Stations: ['DEBY006', 'DEBY007', 'DEBY008', 'DEBY099', 'DEBY110', 'DEBY150'] in the time between 2019-04-01T00:00:00.000Z and now\n",
      "Data will be uploaded\n",
      "To abort type 'no': \n"
     ]
    }
   ],
   "source": [
    "#file = pd.read_excel('metadata/Bericht_EU_Meta_Stationen.xlsx')\n",
    "listofstations=list(file[\"station_code\"])\n",
    "userinput = input(\"Please enter UBA Station to parse: \")\n",
    "\n",
    "parselist=[]\n",
    "#for station in listofstations:\n",
    "#    if userinput in station:\n",
    "#        parselist.append(station)\n",
    "                         \n",
    "if isinstance(userinput,list) is True: #if the user gives an array of station codes, take that\n",
    "    parselist=userinput\n",
    "else: \n",
    "    for nr in range(len(file)): #else perform a substring search in all informations on the stations\n",
    "        the_dict=file.iloc[[str(nr)]].to_dict(orient=\"index\")[nr]\n",
    "        for key in the_dict:\n",
    "            if userinput in str(the_dict[key]):\n",
    "                parselist.append(the_dict[\"station_code\"])\n",
    "\n",
    "print(\"Will parse the following stations: \")\n",
    "print(parselist)\n",
    "\n",
    "\n",
    "print(\"If you do not input a start time, the earliest date a measurement appears will be taken. \")\n",
    "starttimeyesno = input(\"Input start time (yes/no) : \")\n",
    "\n",
    "if starttimeyesno == \"yes\":\n",
    "    yr = input(\"Enter start time year : \")\n",
    "    mth = input(\"Enter start time month : \")\n",
    "    day = input(\"Enter start time day : \")\n",
    "    hr = input(\"Enter start time hour : \")\n",
    "    mts = input(\"Enter start time minutes : \")\n",
    "    starttime = yr + \"-\" + mth + \"-\" + day + \"T\" + hr + \":\" + mts + \":00.000Z\"\n",
    "    starttimestring = starttime\n",
    "else:\n",
    "    starttime = False\n",
    "    starttimestring = \"the first measurement\"\n",
    "\n",
    "\n",
    "print(\"If you do not input an end time, 'right now' will be taken. \")\n",
    "endtimeyesno = input(\"Input end time (yes/no) : \")\n",
    "\n",
    "if endtimeyesno == \"yes\":\n",
    "    yr = input(\"Enter end time year : \")\n",
    "    mth = input(\"Enter end time month : \")\n",
    "    day = input(\"Enter end time day : \")\n",
    "    hr = input(\"Enter end time hour : \")\n",
    "    mts = input(\"Enter end time minutes : \")\n",
    "    endtime = yr + \"-\" + mth + \"-\" + day + \"T\" + hr + \":\" + mts + \":00.000Z\"\n",
    "    endtimestring = endtime\n",
    "else:\n",
    "    endtime = False\n",
    "    endtimestring = \"now\"\n",
    "\n",
    "uploadyesno = input(\"Upload data to server after writing to file (yes/no): \")\n",
    "if uploadyesno == \"yes\":\n",
    "    upload=True\n",
    "    uploadstring = \"Data will be uploaded\"\n",
    "else:\n",
    "    upload=False\n",
    "    uploadstring = \"Data will not be uploaded\"\n",
    "    \n",
    "    \n",
    "print(\"Parsing \" + str(len(parselist)) + \" Stations: \" + str(parselist) + \" in the time between \" + str(starttimestring) + \" and \" + str(endtimestring))\n",
    "print(str(uploadstring))\n",
    "yesno = input(\"To abort type 'no': \")\n",
    "if yesno == 'no':\n",
    "    sys.exit(\"Aborting.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Main Parsing Function: Writes config.txt and runs the script generatemetadata() including parseexcel() if upload is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________\n",
      "Parsing Station 1 of 6\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY006 at Augsburg/Königsplatz\n",
      "saqn:op:mcpm10 exists.\n",
      "Converting 'lfu.bayern.de:station_augsburg_koenigsplatz:1975-01:deby006' to hash 'd42cbb8'\n",
      "Converting 'lfu.bayern.de:generic_beta_ray_absorption_sensor' to hash '9682e37'\n",
      "Converting 'lfu.bayern.de:station_augsburg_koenigsplatz:1975-01:deby006:lfu.bayern.de:generic_beta_ray_absorption_sensor::mcpm10' to hash '80af8f6'\n",
      "Building Datastream saqn:ds:80af8f6\n",
      "Parsing end time not properly set. Taking 2019-05-13 15:19:27.704367\n",
      "Measurements of DEBY006 Particulate matter - PM10, first measurement are being parsed from 2019-04-01 00:00:00\n",
      "File for that datastream and observedProperty already exists. \n",
      "Renaming file C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY006_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09.xlsx to C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY006_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09---2019-05-13T15-19-28.xlsx\n",
      "Moving file into archive folder\n",
      "Crosscheck for DEBY006 Particulate matter - PM10, first measurement with 10 random datapoints gave result True\n",
      "Time elapsed: 0 hours 0 minutes 3 seconds\n",
      "Loading file data\\DEBY006_Particulate matter - PM10, first measurement_2019-04-01_2019-05-13.xlsx...\n",
      "Uploading Observations for Datastream iot.id: saqn:ds:80af8f6\n",
      "Uploaded 1022 out of 1023 Observations. Estimating 0 hours 0 minutes 0 seconds remaining \n",
      "__________________________________________________________________________\n",
      "__________________________________________________________________________econds seconds. \n",
      "Parsing Station 2 of 6\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY007 at Augsburg/Bourges-Platz\n",
      "saqn:op:mcpm10 exists.\n",
      "Converting 'lfu.bayern.de:station_augsburg_bourges-platz:1986-08:deby007' to hash '12b713d'\n",
      "Converting 'lfu.bayern.de:generic_nephelometry_and_beta_attenuation_sensor' to hash 'fa2dbc8'\n",
      "Converting 'lfu.bayern.de:station_augsburg_bourges-platz:1986-08:deby007:lfu.bayern.de:generic_nephelometry_and_beta_attenuation_sensor::mcpm10' to hash '4508dfb'\n",
      "Building Datastream saqn:ds:4508dfb\n",
      "Parsing end time not properly set. Taking 2019-05-13 15:23:08.543842\n",
      "Measurements of DEBY007 Particulate matter - PM10, first measurement are being parsed from 2019-04-01 00:00:00\n",
      "File for that datastream and observedProperty already exists. \n",
      "Renaming file C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY007_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09.xlsx to C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY007_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09---2019-05-13T15-23-08.xlsx\n",
      "Moving file into archive folder\n",
      "Crosscheck for DEBY007 Particulate matter - PM10, first measurement with 10 random datapoints gave result True\n",
      "Time elapsed: 0 hours 0 minutes 3 seconds\n",
      "Loading file data\\DEBY007_Particulate matter - PM10, first measurement_2019-04-01_2019-05-13.xlsx...\n",
      "Uploading Observations for Datastream iot.id: saqn:ds:4508dfb\n",
      "Uploaded 1022 out of 1023 Observations. Estimating 0 hours 0 minutes 0 seconds remaining \n",
      "__________________________________________________________________________\n",
      "__________________________________________________________________________econds seconds. \n",
      "Parsing Station 3 of 6\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY008 at Augsburg/Haunstetten\n",
      "saqn:op:mcpm10 exists.\n",
      "Converting 'lfu.bayern.de:station_augsburg_haunstetten:1975-01:deby008' to hash '504a3c6'\n",
      "Station DEBY008 does not measure PM10\n",
      "Time elapsed: 0 hours 0 minutes 0 seconds\n",
      "No file to parse found that matches the requirements\n",
      "Could not retrieve any data for station DEBY008\n",
      "__________________________________________________________________________\n",
      "Parsing Station 4 of 6\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY099 at Augsburg/LfU\n",
      "saqn:op:mcpm10 exists.\n",
      "Converting 'lfu.bayern.de:station_augsburg_lfu:2000-07:deby099' to hash 'b666034'\n",
      "Converting 'lfu.bayern.de:generic_beta_ray_absorption_sensor' to hash '9682e37'\n",
      "Converting 'lfu.bayern.de:station_augsburg_lfu:2000-07:deby099:lfu.bayern.de:generic_beta_ray_absorption_sensor::mcpm10' to hash '6880098'\n",
      "Building Datastream saqn:ds:6880098\n",
      "Parsing end time not properly set. Taking 2019-05-13 15:25:42.803546\n",
      "Measurements of DEBY099 Particulate matter - PM10, first measurement are being parsed from 2019-04-01 00:00:00\n",
      "File for that datastream and observedProperty already exists. \n",
      "Renaming file C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY099_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09.xlsx to C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY099_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09---2019-05-13T15-25-43.xlsx\n",
      "Moving file into archive folder\n",
      "Crosscheck for DEBY099 Particulate matter - PM10, first measurement with 10 random datapoints gave result True\n",
      "Time elapsed: 0 hours 0 minutes 3 seconds\n",
      "Loading file data\\DEBY099_Particulate matter - PM10, first measurement_2019-04-01_2019-05-13.xlsx...\n",
      "Uploading Observations for Datastream iot.id: saqn:ds:6880098\n",
      "Uploaded 1022 out of 1023 Observations. Estimating 0 hours 0 minutes 0 seconds remaining \n",
      "__________________________________________________________________________\n",
      "__________________________________________________________________________econds seconds. \n",
      "Parsing Station 5 of 6\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY110 at Augsburg/Karlstraße\n",
      "saqn:op:mcpm10 exists.\n",
      "Converting 'lfu.bayern.de:station_augsburg_karlstrasse:2003-08:deby110' to hash '4049564'\n",
      "Converting 'lfu.bayern.de:generic_nephelometry_and_beta_attenuation_sensor' to hash 'fa2dbc8'\n",
      "Converting 'lfu.bayern.de:station_augsburg_karlstrasse:2003-08:deby110:lfu.bayern.de:generic_nephelometry_and_beta_attenuation_sensor::mcpm10' to hash 'b88cfcb'\n",
      "Building Datastream saqn:ds:b88cfcb\n",
      "Parsing end time not properly set. Taking 2019-05-13 15:30:43.075945\n",
      "Measurements of DEBY110 Particulate matter - PM10, first measurement are being parsed from 2019-04-01 00:00:00\n",
      "File for that datastream and observedProperty already exists. \n",
      "Renaming file C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY110_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09.xlsx to C:\\Users\\Paul\\UBA2SensorThings/data\\DEBY110_Particulate matter - PM10, first measurement_2019-01-01_2019-04-09---2019-05-13T15-30-43.xlsx\n",
      "Moving file into archive folder\n",
      "Crosscheck for DEBY110 Particulate matter - PM10, first measurement with 10 random datapoints gave result True\n",
      "Time elapsed: 0 hours 0 minutes 3 seconds\n",
      "Loading file data\\DEBY110_Particulate matter - PM10, first measurement_2019-04-01_2019-05-13.xlsx...\n",
      "Uploading Observations for Datastream iot.id: saqn:ds:b88cfcb\n",
      "Uploaded 1022 out of 1023 Observations. Estimating 0 hours 0 minutes 0 seconds remaining \n",
      "__________________________________________________________________________\n",
      "__________________________________________________________________________econds seconds. \n",
      "Parsing Station 6 of 6\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY150 at DBS Augsburg\n",
      "saqn:op:mcpm10 exists.\n",
      "Converting 'lfu.bayern.de:station_dbs_augsburg:1998-07:deby150' to hash '87704c8'\n",
      "Could not retrieve any data for station DEBY150\n",
      "\n",
      "__________________________________________________________________________\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for station in parselist:\n",
    "    \n",
    "    print(\"__________________________________________________________________________\")\n",
    "    print(\"Parsing Station \" + str(parselist.index(station) + 1) + \" of \" + str(len(parselist)))\n",
    "    with open('config.txt','w') as configtxt:\n",
    "        configuration={}\n",
    "\n",
    "        configuration[\"url\"] = destinationurl\n",
    "        configuration[\"thingcode\"] = station\n",
    "        configuration[\"feature\"] = \"PM10\"\n",
    "        configuration[\"intervalllength\"] = \"PT1H\"\n",
    "        configuration[\"runparseaftermodelling\"] = upload\n",
    "        configuration[\"starttime\"] = starttime #in case of false will try to find the earliest date of a measurement\n",
    "        configuration[\"endtime\"] = endtime #in case of false will take utctime.now()\n",
    "        #time format is ISO8601 time standard UTC, e.g.: '2018-12-31T23:59:00.000Z'\n",
    "\n",
    "        configtxt.write(json.dumps(configuration))\n",
    "    \n",
    "    generatemetadata()\n",
    "#    with open('UBA_generate_excel_with_metadata.py') as generatedata:\n",
    "#        exec(generatedata.read())\n",
    "\n",
    "sys.stdout.write('\\n' + '__________________________________________________________________________' + '\\n')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.delete(\"http://smartaqnet-dev.teco.edu/v1.0/Things('d42cbb8')\")\n",
    "# requests.delete(\"http://smartaqnet-dev.teco.edu/v1.0/Things('12b713d')\")\n",
    "# requests.delete(\"http://smartaqnet-dev.teco.edu/v1.0/Things('504a3c6')\")\n",
    "# requests.delete(\"http://smartaqnet-dev.teco.edu/v1.0/Things('b666034')\")\n",
    "# requests.delete(\"http://smartaqnet-dev.teco.edu/v1.0/Things('4049564')\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Sensors\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Locations\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/ObservedProperties\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/FeaturesOfInterest\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Datastreams\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

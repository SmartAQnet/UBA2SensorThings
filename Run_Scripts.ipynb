{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter UBA Station to parse: DEBY110\n",
      "If you do not input a start time, the earliest date a measurement appears will be taken. \n",
      "Input start time (yes/no) : no\n",
      "If you do not input an end time, 'right now' will be taken. \n",
      "Input end time (yes/no) : yes\n",
      "Enter end time year : 2018\n",
      "Enter end time month : 12\n",
      "Enter end time day : 31\n",
      "Enter end time hour : 23\n",
      "Enter end time minutes : 55\n",
      "Upload data to server after parsing (yes/no): yes\n",
      "Parsing 1 Stations: ['DEBY110'] in the time between the first measurement and 2018-12-31T23:55:00.000Z\n",
      "Data will be uploaded\n",
      "To abort type 'no': \n",
      "__________________________________________________________________________\n",
      "Parsing Station 1 of 1\n",
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n",
      "Check: Going to uploading data for station DEBY110 at Augsburg/Karlstraße\n",
      "Building Datastream saqn:ds:lfu.bayern.de:generic_nephelometry_and_beta_attenuation_sensor:deby110:pm10\n",
      "Searching Datastream Start Time..............................................\n",
      "Parsing start time not properly set. Taking earliest date measurements appear: 2016-12-31 00:00:00\n",
      "Measurements of DEBY110 Particulate matter - PM10, first measurement are being parsed from 2016-12-31 00:00:00\n",
      "Crosscheck for DEBY110 Particulate matter - PM10, first measurement with 10 random datapoints gave result True\n",
      "Time elapsed: 0 hours 0 minutes 43 seconds\n",
      "Loading file data\\DEBY110_Particulate matter - PM10, first measurement_2016-12-31_2018-12-31.xlsx...\n",
      "Uploading Observations for Datastream iot.id: saqn:ds:lfu.bayern.de:generic_nephelometry_and_beta_attenuation_sensor:deby110:pm10\n",
      "Donehed! Successfully uploaded 17543 Observations in 0 hours 8 minutes 53 seconds seconds.  \n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import sys\n",
    "import time\n",
    "import requests, json\n",
    "import numpy as np\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import random\n",
    "import glob\n",
    "\n",
    "\n",
    "#converts numpy types to python types, otherwise json conversion produces an error. call json.dumps(***, cls=MyEncoder)\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file = pd.read_excel('metadata/Bericht_EU_Meta_Stationen.xlsx')\n",
    "filemeta=pd.read_excel('metadata/Bericht_EU_Meta_Stationsparameter.xlsx')\n",
    "df_stationparameters = filemeta.set_index(\"station_code\")\n",
    "\n",
    "#federal state, network code, website\n",
    "ccodesetreadable=[('Hessen', 'DE009A','hlnug.de'),\n",
    " ('Saarland', 'DE001A','saarland.de'),\n",
    " ('Berlin', 'DE008A','berlin.de'),\n",
    " ('Bayern', 'DE007A','lfu.bayern.de'),\n",
    " ('Rheinland-Pfalz', 'DE011A','luft.rlp.de'),\n",
    " ('Sachsen', 'DE016A','umwelt.sachsen.de'),\n",
    " ('Umweltbundesamt', 'DE006A','umweltbundesamt.de'),\n",
    " ('Baden-Wuerttemberg', 'DE005A','lubw.baden-wuerttemberg.de'),\n",
    " ('Nordrhein-Westfalen', 'DE004A','lanuv.nrw.de'),\n",
    " ('Brandenburg', 'DE014A','lfu.brandenburg.de'),\n",
    " ('Bremen', 'DE013A','bauumwelt.bremen.de'),\n",
    " ('Mecklenburg-Vorpommern', 'DE018A','lung.mv-regierung.de'),\n",
    " ('Hamburg', 'DE012A','luft.hamburg.de'),\n",
    " ('Thueringen', 'DE017A','tlug-jena.de'),\n",
    " ('Schleswig-Holstein', 'DE002A','schleswig-holstein.de'),\n",
    " ('Sachsen-Anhalt', 'DE015A','luesa.sachsen-anhalt.de'),\n",
    " ('Niedersachsen', 'DE010A','umwelt.niedersachsen.de')]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#returns the url for the network code\n",
    "def repnetcodebyurl(netcode):\n",
    "    for item in ccodesetreadable:\n",
    "        if item[1]==netcode:\n",
    "            return(item[2])\n",
    "\n",
    "#returns the name of the state for the network code\n",
    "def repnetcodebystate(netcode):\n",
    "    for item in ccodesetreadable:\n",
    "        if item[1]==netcode:\n",
    "            return(item[0])\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#functions for time conversion\n",
    "def readtime(sec):\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return(str(int(h)) + \" hours \" + str(int(m)) + \" minutes \" + str(int(s)) + \" seconds\")\n",
    "\n",
    "def tounixtime(datetime_input):\n",
    "    return(calendar.timegm(datetime_input.utctimetuple()))\n",
    "\n",
    "def todatetimeformat(utctime):\n",
    "    year=int(utctime[0])*1000 + int(utctime[1])*100 + int(utctime[2])*10 + int(utctime[3])\n",
    "    month=int(utctime[5])*10 + int(utctime[6])\n",
    "    day=int(utctime[8])*10 + int(utctime[9])\n",
    "    hr=int(utctime[11])*10 + int(utctime[12])\n",
    "    minute=int(utctime[14])*10 + int(utctime[15])\n",
    "    second=int(utctime[17])*10 + int(utctime[18])\n",
    "    millisecond=int(utctime[20])*100 + int(utctime[21])*10 + int(utctime[22])   \n",
    "    return(datetime(year,month,day,hr,minute,second,millisecond))\n",
    "\n",
    "def toutcformat(datetime_input):\n",
    "    tstr=str(datetime_input)\n",
    "    year=tstr[0]+tstr[1]+tstr[2]+tstr[3]\n",
    "    month=tstr[5]+tstr[6]\n",
    "    day=tstr[8]+tstr[9]\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[11]+tstr[12]))==int:\n",
    "            hour=str(tstr[11]+tstr[12])\n",
    "    except:\n",
    "        hour='00'             #no hours given       \n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[14]+tstr[15]))==int:\n",
    "            minute=str(tstr[14]+tstr[15])\n",
    "    except:\n",
    "        minute='00'             #no minutes given\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[17]+tstr[18]))==int:\n",
    "            second=str(tstr[17]+tstr[18])\n",
    "    except:\n",
    "        second='00'             #no seconds given\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[20]+tstr[21]+tstr[22]))==int:\n",
    "            millisecond=str(tstr[20]+tstr[21]+tstr[22])\n",
    "    except:\n",
    "        millisecond='000'       #no milliseconds given\n",
    "\n",
    "\n",
    "\n",
    "    utctime=year + '-' + month + '-' + day + 'T' + hour + ':' + minute + ':' + second + '.' + millisecond + 'Z'\n",
    "    return utctime\n",
    "\n",
    "def addminutesutc(utctime,mins):\n",
    "    return(toutcformat(datetime.utcfromtimestamp(tounixtime(todatetimeformat(utctime))+(mins*60))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def idstr(idinput):\n",
    "    return(str(idinput).lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"ä\", \"ae\").replace(\"ö\", \"oe\").replace(\"ü\", \"ue\").replace(\"ß\", \"ss\"))\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#function that builds the metadata\n",
    "\n",
    "\n",
    "def generatemetadata():\n",
    "\n",
    "    #input values. Read config.txt\n",
    "\n",
    "    try:\n",
    "        with open('config.txt') as configfile:\n",
    "            config=json.loads(configfile.read())\n",
    "\n",
    "            url = config['url']\n",
    "            thingcode = config['thingcode']\n",
    "            feature = config['feature']\n",
    "        #    intervalllength = config['intervalllength']\n",
    "            if config[\"runparseaftermodelling\"] == True:\n",
    "                upload == True #set to True/False to enable/disable upload of metadata to FROST\n",
    "    except:\n",
    "        sys.exit(\"Config File not properly set!\")\n",
    "\n",
    "    baseurl = 'https://www.umweltbundesamt.de/js/uaq/data/stations' #get data where from\n",
    "    scope='1SMW' #umweltbundesamt website code: 1 hour means\n",
    "    scopesec= 60*60 #scope in seconds, needed for interval to tag the next observation\n",
    "\n",
    "    print(\"Upload is set to \" + str(upload))\n",
    "    print(\"Extracting Observations for ObservedProperty \" + str(feature))\n",
    "\n",
    "    #list of features with their component codes as appears in 'metadata/Bericht_EU_Meta_Stationsparameter.xlsx'\n",
    "    listofreplacements=[\n",
    "        ('PM10','5.0'),\n",
    "        ('PM2,5','6001.0'),\n",
    "        ('PM1','6002.0'),\n",
    "    ]\n",
    "\n",
    "    #fetches the code for the above defined 'feature'\n",
    "    for eachelement in listofreplacements:\n",
    "        if eachelement[0] == feature:\n",
    "            code = eachelement[1]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #crosscheck\n",
    "    print('Check: Going to uploading data for station ' + str(thingcode) + ' at ' + str(file.set_index(\"station_code\")[\"station_name\"][thingcode]))\n",
    "\n",
    "\n",
    "\n",
    "#    currentyear=str(datetime.utcnow())[0:4]\n",
    "\n",
    "\n",
    "\n",
    "    #metadata\n",
    "    totalstarttime = time.time() #to check how long the upload took\n",
    "    \n",
    "    #observed property --> should already be \"officially\" created!\n",
    "    generatepropertyid= \"saqn:op:\" + str(feature).lower().replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "    obsproperty = {\n",
    "        \"name\": str(feature),\n",
    "        \"description\": str(feature),\n",
    "        \"definition\": \"\",\n",
    "        \"@iot.id\": idstr(generatepropertyid)\n",
    "        }\n",
    "    if upload==True:\n",
    "        requests.post(url + '/ObservedProperties', json.dumps(obsproperty))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    thingnr=list(file[\"station_code\"]).index(thingcode) #the number of the row in the excel file\n",
    "\n",
    "    generatedescr=\"\" #generates the description for the thing\n",
    "    if  df_stationparameters[\"type_of_parameter\"].index.contains(thingcode): #checks whether the station actually exists\n",
    "        if thingcode in df_stationparameters[\"type_of_parameter\"][thingcode]: #if the stations measures only one type of parameter, the index is not returned somehow and the loop produces an error, therefore check if the index is returned and if not handle case separately\n",
    "            for element in list(set(df_stationparameters[\"type_of_parameter\"][thingcode])):\n",
    "                generatedescr+= \" -\" + element + \"-\"\n",
    "        else: #if the station only measures one type of parameter (loop produces an error in that case, thus handled separately)\n",
    "            generatedescr+= \" -\" + df_stationparameters[\"type_of_parameter\"][thingcode] + \"-\"\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    #building things\n",
    "\n",
    "    #Location ID\n",
    "    generatelocid = \"geo:\" + str(float(file[\"station_latitude_d\"][thingnr])) + \",\" + str(float(file[\"station_longitude_d\"][thingnr])) + \",\" + str(float(file[\"station_altitude\"][thingnr]))\n",
    "\n",
    "    #Thing ID\n",
    "    generatethingid=\"saqn:t\" #generates the id for the thing by adding each of the following features to define it uniquely\n",
    "    generatethingid+=\":\" + str(repnetcodebyurl(file[\"network_code\"][thingnr])) #adds the url through the network code\n",
    "    generatethingid+=\":\" + \"station_\" + str(file[\"station_name\"][thingnr])\n",
    "    generatethingid+=\":\" + str(file[\"station_start_date\"][thingnr])[0:4] + \"-\" + str(file[\"station_start_date\"][thingnr])[4:6]\n",
    "    generatethingid+=\":\" + str(file[\"station_code\"][thingnr])\n",
    "#    generatethingid+=\":\" + str(currentyear) #adds the current year to make the identifier unique in case the id gets changed one day\n",
    "#    for label in [\"station_name\",\"station_start_date\",\"station_code\"]: #add more info if desired\n",
    "#        generatethingid+=\":\" + str(file[label][thingnr])\n",
    "\n",
    "    #generates a dictionary of all raw properties of the thing\n",
    "    rawproperties = {}\n",
    "    for eachproperty in list(file):\n",
    "        if str(file[eachproperty][thingnr])=='nan':\n",
    "            rawproperties[eachproperty]='nan'\n",
    "        else:\n",
    "            rawproperties[eachproperty] = file[eachproperty][thingnr]\n",
    "\n",
    "    #generate the thing JSON\n",
    "    thingdata = {\"name\": \"Measuring Station \" + str(file[\"station_name\"][thingnr]),\n",
    "        \"description\": \"A station measuring\" + str(generatedescr),\n",
    "        \"properties\": rawproperties,\n",
    "        \"@iot.id\": idstr(generatethingid),\n",
    "         \"Locations\": [{\n",
    "            \"name\": \"Location at latitude \" + str(float(file[\"station_latitude_d\"][thingnr])) + \" and longitude \" + str(float(file[\"station_longitude_d\"][thingnr])),\n",
    "            \"description\": \"located at \" + str(file[\"station_name\"][thingnr]),\n",
    "            \"encodingType\": \"application/vnd.geo+json\",\n",
    "            \"@iot.id\": idstr(generatelocid),\n",
    "            \"location\": {\n",
    "                  \"type\": \"Point\",\n",
    "                  \"coordinates\": [float(file[\"station_longitude_d\"][thingnr]), float(file[\"station_latitude_d\"][thingnr]), float(file[\"station_altitude\"][thingnr])]\n",
    "            }\n",
    "\n",
    "          }]\n",
    "    }\n",
    "    if upload==True:\n",
    "        requests.post(url + '/Things', json.dumps(thingdata, cls=MyEncoder))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    #loop over all sensors and check which contains the requested observedproperty\n",
    "    try:\n",
    "        if float(code) not in list(df_stationparameters[\"component_code\"][thingcode]):\n",
    "            print(\"Station \" + str(thingcode) + \" does not measure \" + str(feature))\n",
    "        else:\n",
    "            for sensnr in range(len(list(df_stationparameters[\"component_code\"][thingcode]))):\n",
    "                if list(df_stationparameters[\"component_code\"][thingcode])[sensnr] == float(code):\n",
    "                    thissensor=list(df_stationparameters[\"parameter\"][thingcode])[sensnr] #the parameter to parse, e.g. \"Particulate Matter - PM10, first measurement\"\n",
    "                    mestech=list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]\n",
    "                    #warning: if only one sensor exists, this will blow up because one item is not returned as dataframe but as string\n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------\n",
    "                    #building the sensors\n",
    "                    #generates a dictionary of all raw properties of the thing to dump into metadata property\n",
    "                    uploadmetadata = {}             \n",
    "                    uploadmetadata[\"script version number\"]=\"00000\"\n",
    "                    uploadmetadata[\"uploaded data from\"]=configuration[\"starttime\"]\n",
    "                    uploadmetadata[\"uploaded data until\"]=configuration[\"endtime\"]\n",
    "                    \n",
    "                    rawmetadata = {}\n",
    "                    rawmetadata[\"station_code\"]=thingcode\n",
    "                    rawmetadata[\"upload properties\"]=uploadmetadata\n",
    "                    \n",
    "                    \n",
    "                    for eachdata in list(df_stationparameters): #option 1: all metadata\n",
    "                    #for eachdata in [\"type_of_parameter\",\"parameter\",\"component_code\",\"measurement_technique_principle\"]: #option 2: pick\n",
    "                        if str(list(df_stationparameters[eachdata][thingcode])[sensnr]) == 'nan':\n",
    "                            rawmetadata[eachdata]='nan'\n",
    "                        else:\n",
    "                            rawmetadata[eachdata] = list(df_stationparameters[eachdata][thingcode])[sensnr]\n",
    "\n",
    "\n",
    "                    #Sensor ID - example: saqn:s:lfu.bayern.de:particulate_matter_-_pm10_first_measurement:nephelometry_and_beta_attenuation:2019:deby007\n",
    "\n",
    "                    generatesensorid=\"saqn:s\" #generates the id for the sensor by adding each of the following features to define it uniquely\n",
    "                    generatesensorid+=\":\" + str(repnetcodebyurl(file[\"network_code\"][thingnr])) #adds the url through the network code\n",
    "                    generatesensorid+=\":\" + \"generic_\" + str(list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]) + \"_sensor\"\n",
    "\n",
    "\n",
    "        #                for label in [\"parameter\",\"measurement_technique_principle\"]: #add more info if desired\n",
    "        #                    generatesensorid+=\":\" + str(list(df_stationparameters[label][thingcode])[sensnr])\n",
    "        #                generatesensorid+=\":\" + str(currentyear) #adds the current year to make the identifier unique in case the id gets changed one day\n",
    "        #                generatesensorid+=\":\" + str(thingcode) #code of the corresponding station\n",
    "\n",
    "                    #generate sensor JSON\n",
    "                    sensor = {\"name\": \"A \" + str(feature) + \" sensor\",\n",
    "                            \"description\": \"A sensor measuring \" + str(feature) + \" using \" + str(mestech),\n",
    "                            \"encodingType\": \"application/json\",\n",
    "                            \"metadata\": \"\",\n",
    "                            \"@iot.id\": idstr(generatesensorid)\n",
    "                            }\n",
    "                    if upload==True:\n",
    "                        requests.post(url + '/Sensors', json.dumps(sensor, cls=MyEncoder))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------\n",
    "                    #building the datastreams\n",
    "\n",
    "                    generatestreamid = \"saqn:ds\"\n",
    "                    generatestreamid+=\":\" + str(repnetcodebyurl(file[\"network_code\"][thingnr]))\n",
    "                    generatestreamid+=\":\" + \"generic_\" + str(list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]) + \"_sensor\"\n",
    "#                    generatestreamid+=\":\" + str(list(df_stationparameters[\"measurement_start_date\"][thingcode])[sensnr])[0:4] + \"-\" + str(list(df_stationparameters[\"measurement_start_date\"][thingcode])[sensnr])[4:6]\n",
    "                    generatestreamid+=\":\" + str(thingcode)\n",
    "                    generatestreamid+=\":\" + str(feature)\n",
    "\n",
    "                    print(\"Building Datastream \" + idstr(generatestreamid))\n",
    "\n",
    "                    datastream = {\"name\": str(thissensor) + \" Datastream of station \" + str(thingcode),\n",
    "                                \"description\": \"A Datastream measuring \" + str(thissensor) + \" using \" + str(mestech),\n",
    "                                \"observationType\": \"\",\n",
    "                                \"unitOfMeasurement\": {\n",
    "                                    \"name\": \"microgram per cubic meter\",\n",
    "                                    \"symbol\": \"ug/m^3\",\n",
    "                                    \"definition\": \"none\"\n",
    "                                    },\n",
    "                                \"properties\": rawmetadata,\n",
    "                                \"@iot.id\": idstr(generatestreamid),\n",
    "                                \"Thing\":{\"@iot.id\":idstr(generatethingid)},\n",
    "                                \"Sensor\":{\"@iot.id\":idstr(generatesensorid)},\n",
    "                                \"ObservedProperty\":{\"@iot.id\":idstr(generatepropertyid)}\n",
    "                                }\n",
    "\n",
    "                    if upload==True:\n",
    "                        requests.post(url + '/Datastreams', json.dumps(datastream))\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    #END OF METADATA - - - BEGIN OF DATA UPLOAD\n",
    "                    #------------------------------------------------------------\n",
    "\n",
    "                    def graburl(start,end): #start and end in datetime\n",
    "                        return(baseurl + '/measuring?pollutant[]=' + feature + '&scope[]=' + scope + '&station[]=' + thingcode + '&group[]=pollutant&range[]=' + str(tounixtime(start)) + ',' + str(tounixtime(end)))\n",
    "\n",
    "                    #begin time\n",
    "                    try:\n",
    "                        begintime=todatetimeformat(config['starttime'])\n",
    "                    except:\n",
    "\n",
    "                        #get the start month of the respective datastream\n",
    "\n",
    "                        def getstarttime():\n",
    "                            sys.stdout.write(\"Searching Datastream Start Time\")\n",
    "                            for i in range(1970,2018+1): #ranges to 2018, the +1 is because how range counts\n",
    "                                try:\n",
    "                                    theurl=graburl(datetime(i,1,1,0,0,0),datetime(i,12,31,23,59,0))\n",
    "                                    datafromurl=json.loads(requests.get(theurl).content)[\"data\"][0]\n",
    "                                    if datafromurl[0][0]!='bananas': #anything but an error\n",
    "                                        for j in range(1,12+1):\n",
    "                                            try:\n",
    "                                                theurl2=graburl(datetime(i,j,1,0,0,0),datetime(i,j,calendar.monthrange(i,j)[1],23,59,0))\n",
    "                                                datafromurl2=json.loads(requests.get(theurl2).content)[\"data\"][0]\n",
    "                                                if datafromurl2[0][0]!='bananas':\n",
    "                                                    for k in range(1,calendar.monthrange(i,j)[1]+1):\n",
    "                                                        try:\n",
    "                                                            theurl3=graburl(datetime(i,j,k,0,0,0),datetime(i,j,k,23,59,0))\n",
    "                                                            datafromurl3=json.loads(requests.get(theurl3).content)[\"data\"][0]\n",
    "                                                            if datafromurl3[0][0]!='bananas':\n",
    "                                                                return(datetime(i,j,k,0,0,0))\n",
    "                                                                break\n",
    "                                                        except:\n",
    "                                                            pass\n",
    "                                            except:\n",
    "                                                pass\n",
    "                                except:\n",
    "                                    sys.stdout.write(\".\")\n",
    "                                    pass\n",
    "\n",
    "                        begintime=getstarttime()\n",
    "\n",
    "                        print(\"\")\n",
    "                        print(\"Parsing start time not properly set. Taking earliest date measurements appear: \" + str(begintime))\n",
    "\n",
    "                    #end time\n",
    "                    try:\n",
    "                        endtime=todatetimeformat(config['endtime'])\n",
    "                    except:\n",
    "                        print(\"Parsing end time not properly set. Taking \" + str(datetime.utcnow()))\n",
    "                        endtime = datetime.utcnow() \n",
    "\n",
    "\n",
    "                    #------------------------------------------------------------\n",
    "\n",
    "                    try:\n",
    "                        begintimeunix=tounixtime(begintime)\n",
    "                        endtimeunix=tounixtime(endtime)\n",
    "                    except:\n",
    "                        print(\"Error! Datatstream empty! Cannot find any data!\")\n",
    "                        return()\n",
    "\n",
    "                    print('Measurements of ' + str(thingcode) + ' ' +  str(thissensor) + ' are being parsed from ' + str(begintime))\n",
    "\n",
    "\n",
    "                    getdatafrom=baseurl + '/measuring?pollutant[]=' + feature + '&scope[]=' + scope + '&station[]=' + thingcode + '&group[]=pollutant&range[]=' + str(begintimeunix + (scopesec/2)) + ',' + str(endtimeunix  + (scopesec/2))\n",
    "                    datavalue=json.loads(requests.get(getdatafrom).content)[\"data\"][0]\n",
    "\n",
    "\n",
    "                    #convert list into a dataframe\n",
    "                    datalist=[]\n",
    "                    labels=['interval_start_time','interval_end_time',str(feature)]\n",
    "\n",
    "                    for i in range(len(datavalue)):\n",
    "                            datalist.append([toutcformat(datetime.utcfromtimestamp(begintimeunix + (scopesec*i))),toutcformat(datetime.utcfromtimestamp(begintimeunix + ((scopesec*i) + (scopesec-60)) )),datavalue[i][0]])\n",
    "\n",
    "                    dataframe = pd.DataFrame.from_records(datalist, columns=labels)\n",
    "                    dataframemeta = pd.DataFrame.from_records([[idstr(generatestreamid)]], columns=['datastreamID'])\n",
    "\n",
    "                    #save dataframe to excel sheet\n",
    "                    filename= str(thingcode) + \"_\" + str(thissensor) + \"_\" + str(toutcformat(begintime)[0:10]) + \"_\" + str(toutcformat(endtime)[0:10]) + \".xlsx\"\n",
    "                    writer = ExcelWriter('data/' +  str(filename))\n",
    "                    dataframe.to_excel(writer,'Sheet1',index=False)\n",
    "                    dataframemeta.to_excel(writer,'id',index=False)\n",
    "                    writer.save()\n",
    "\n",
    "                    #crosschecking: checks a number of random points between the dataframe and the url\n",
    "\n",
    "                    #the UBA database gives the result for the hour that has PASSED, e.g. the value at 5 o'clock is the mean between 4 and 5 o'clock\n",
    "                    #the code writes a value for the interval between e.g. 3:00 and 3:59. for this interval, the UBA value at 3:30 is taken. \n",
    "                    #therefore the check also needs to add 30m to the url to check with the corresponding start time\n",
    "\n",
    "                    numberoftests=10\n",
    "                    testistrue=False\n",
    "\n",
    "\n",
    "                    for i in range(0,10):\n",
    "                        r = random.randint(1,len(list(dataframe[\"interval_start_time\"])))\n",
    "                        getdatapoint=graburl(todatetimeformat(addminutesutc(dataframe[\"interval_start_time\"][r],30)),todatetimeformat(addminutesutc(dataframe[\"interval_end_time\"][r],30)))\n",
    "                        checkdatavalue=json.loads(requests.get(getdatapoint).content)[\"data\"][0][0][0]\n",
    "                        if dataframe[feature][r] == checkdatavalue:\n",
    "                            testistrue=True\n",
    "                        else:\n",
    "                            print(str(dataframe[feature][r]) + \"!=\" + str(checkdatavalue))\n",
    "                            testistrue=False\n",
    "\n",
    "                    print(\"Crosscheck for \" + str(thingcode) + \" \" + str(thissensor) + \" with \" + str(numberoftests) + \" random datapoints gave result \" + str(testistrue))\n",
    "\n",
    "\n",
    "        endtime=time.time()\n",
    "        timeelapsed=endtime-totalstarttime\n",
    "\n",
    "        print(\"Time elapsed: \" + str(readtime(timeelapsed)))\n",
    "\n",
    "        if upload==True:\n",
    "            parseexcel()\n",
    "\n",
    "    except:\n",
    "        print(\"Could not retrieve any data for station \" + str(thingcode))\n",
    "    return()\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#function parses the data file\n",
    "\n",
    "\n",
    "\n",
    "def parseexcel():\n",
    "    #input parameters. Read config.txt\n",
    "\n",
    "    try:\n",
    "        with open('config.txt') as configfile:\n",
    "            config=json.loads(configfile.read())\n",
    "\n",
    "            url = config['url']\n",
    "            thingcode = config['thingcode']\n",
    "            feature = config['feature']\n",
    "            intervalllength = config['intervalllength']\n",
    "    except:\n",
    "        sys.exit(\"Config File not properly set!\")\n",
    "\n",
    "\n",
    "    #read data and auxiliary functions\n",
    "\n",
    "    #get the component code for a feature as long as it is listed in the listofreplacements\n",
    "    def componentcode(feat):\n",
    "        for rep in listofreplacements:\n",
    "            if rep[0]==feature:\n",
    "                return(rep[1])\n",
    "\n",
    "\n",
    "    try: #get the first file matching thingcode and feature e.g. DEBY007_PM10_*.xlsx\n",
    "        filename = glob.glob(\"data/\" + str(thingcode) + \"*\" + str(feature) + \"*.xlsx\")[0]\n",
    "    except:\n",
    "        return()\n",
    "    \n",
    "    print(\"Loading file \" + str(filename) + \"...\")\n",
    "    datafile=pd.read_excel(str(filename),0)\n",
    "    datafilemeta=pd.read_excel(str(filename),'id')\n",
    "\n",
    "    #clear all nullresults (False) before the first measurement\n",
    "    nullresults=0\n",
    "    for i in range(len(list(datafile[\"interval_start_time\"]))):\n",
    "        if datafile[feature][i]==0:\n",
    "            nullresults+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #read file again but skip the nullresults\n",
    "    datafile=pd.read_excel(str(filename),skiprows=range(1,nullresults+1))\n",
    "\n",
    "    #for displaying the elapsed time\n",
    "    starttime=time.time()\n",
    "\n",
    "\n",
    "\n",
    "#    currentyear=str(datetime.utcnow())[0:4]\n",
    "\n",
    "\n",
    "\n",
    "    #get datastream id\n",
    "    #datastreamID=\"saqn:d:\" + str(repnetcodebyurl(file[\"network_code\"][thingnr])) + \":\" + str(list(df_stationparameters[\"measurement_start_date\"][thingcode])[sensnr])[0:6] + \":\" + thingcode + \":\" + feature.replace(\" \", \"_\")\n",
    "    datastreamID=datafilemeta[\"datastreamID\"][0]\n",
    "\n",
    "\n",
    "    print(\"Uploading Observations for Datastream iot.id: \" + datastreamID)\n",
    "\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    for i in range(len(list(datafile[\"interval_start_time\"]))): #eleganter ist hier feature zu nehmen aber ist egal\n",
    "        #phenotime = toutcformat(datetime.utcfromtimestamp(tounixtime(begin)+scopesec*i))\n",
    "\n",
    "        generateobsid=\"saqn:o:\" + str(datastreamID)[8:] + \":\" + str(datafile[\"interval_start_time\"][i]) + \"/\" + str(intervalllength)\n",
    "\n",
    "        observation = {\n",
    "        \"phenomenonTime\" : str(datafile[\"interval_start_time\"][i]) + \"/\" + str(intervalllength), \n",
    "        \"result\" : float(datafile[str(feature)][i]),\n",
    "        \"Datastream\":{\"@iot.id\": str(datastreamID)},\n",
    "        \"@iot.id\": idstr(generateobsid)\n",
    "        }\n",
    "\n",
    "        requests.post(url + '/Observations', json.dumps(observation))\n",
    "\n",
    "        #estimating time remaining for parsing\n",
    "        timeelapsed=time.time()-starttime\n",
    "        timeelapsedread=readtime(((len(list(datafile[\"interval_start_time\"]))/(i+1))-1)*int(timeelapsed+1))\n",
    "        sys.stdout.write('Uploaded ' + str(i) + ' out of ' + str(len(list(datafile[\"interval_start_time\"]))) + ' Observations. Estimating ' + timeelapsedread + ' remaining \\r')\n",
    "\n",
    "\n",
    "    endtime=time.time()\n",
    "    timeelapsed=endtime-starttime\n",
    "\n",
    "    sys.stdout.write('Finshed! Successfully uploaded ' + str(len(list(datafile[\"interval_start_time\"]))) + ' Observations in ' + str(readtime(int(timeelapsed))) + ' seconds. \\r')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "datafile = pd.read_excel('metadata/Bericht_EU_Meta_Stationen.xlsx')\n",
    "listofstations=list(datafile[\"station_code\"])\n",
    "userinput = input(\"Please enter UBA Station to parse: \")\n",
    "\n",
    "parselist=[]\n",
    "for station in listofstations:\n",
    "    if userinput in station:\n",
    "        parselist.append(station)\n",
    "\n",
    "print(\"If you do not input a start time, the earliest date a measurement appears will be taken. \")\n",
    "starttimeyesno = input(\"Input start time (yes/no) : \")\n",
    "\n",
    "if starttimeyesno == \"yes\":\n",
    "    yr = input(\"Enter start time year : \")\n",
    "    mth = input(\"Enter start time month : \")\n",
    "    day = input(\"Enter start time day : \")\n",
    "    hr = input(\"Enter start time hour : \")\n",
    "    mts = input(\"Enter start time minutes : \")\n",
    "    starttime = yr + \"-\" + mth + \"-\" + day + \"T\" + hr + \":\" + mts + \":00.000Z\"\n",
    "    starttimestring = starttime\n",
    "else:\n",
    "    starttime = False\n",
    "    starttimestring = \"the first measurement\"\n",
    "\n",
    "\n",
    "print(\"If you do not input an end time, 'right now' will be taken. \")\n",
    "endtimeyesno = input(\"Input end time (yes/no) : \")\n",
    "\n",
    "if endtimeyesno == \"yes\":\n",
    "    yr = input(\"Enter end time year : \")\n",
    "    mth = input(\"Enter end time month : \")\n",
    "    day = input(\"Enter end time day : \")\n",
    "    hr = input(\"Enter end time hour : \")\n",
    "    mts = input(\"Enter end time minutes : \")\n",
    "    endtime = yr + \"-\" + mth + \"-\" + day + \"T\" + hr + \":\" + mts + \":00.000Z\"\n",
    "    endtimestring = endtime\n",
    "else:\n",
    "    endtime = False\n",
    "    endtimestring = \"now\"\n",
    "\n",
    "uploadyesno = input(\"Upload data to server after parsing (yes/no): \")\n",
    "if uploadyesno == \"yes\":\n",
    "    upload=True\n",
    "    uploadstring = \"Data will be uploaded\"\n",
    "else:\n",
    "    upload=False\n",
    "    uploadstring = \"Data will not be uploaded\"\n",
    "    \n",
    "    \n",
    "print(\"Parsing \" + str(len(parselist)) + \" Stations: \" + str(parselist) + \" in the time between \" + str(starttimestring) + \" and \" + str(endtimestring))\n",
    "print(str(uploadstring))\n",
    "yesno = input(\"To abort type 'no': \")\n",
    "if yesno == 'no':\n",
    "    sys.exit(\"Aborting.\")\n",
    "\n",
    "\n",
    "for station in parselist:\n",
    "    \n",
    "    print(\"__________________________________________________________________________\")\n",
    "    print(\"Parsing Station \" + str(parselist.index(station) + 1) + \" of \" + str(len(parselist)))\n",
    "    with open('config.txt','w') as configtxt:\n",
    "        configuration={}\n",
    "\n",
    "        configuration[\"url\"] = \"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0\"\n",
    "        configuration[\"thingcode\"] = station\n",
    "        configuration[\"feature\"] = \"PM10\"\n",
    "        configuration[\"intervalllength\"] = \"PT1H\"\n",
    "        configuration[\"runparseaftermodelling\"] = upload\n",
    "        configuration[\"starttime\"] = starttime #in case of false will try to find the earliest date of a measurement\n",
    "        configuration[\"endtime\"] = endtime #in case of false will take utctime.now()\n",
    "        #time format is ISO8601 time standard UTC, e.g.: '2018-12-31T23:59:00.000Z'\n",
    "\n",
    "        configtxt.write(json.dumps(configuration))\n",
    "    \n",
    "    generatemetadata()\n",
    "#    with open('UBA_generate_excel_with_metadata.py') as generatedata:\n",
    "#        exec(generatedata.read())\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Things\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Sensors\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Locations\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/ObservedProperties\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/FeaturesOfInterest\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Datastreams\")\n",
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.delete(\"http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0/Things('saqn%3At%3Alfu.bayern.de%3Astation_augsburg_karlstrasse%3A2003-08%3Adeby110')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "#converts numpy types to python types, otherwise json conversion produces an error. call json.dumps(***, cls=MyEncoder)\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(MyEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload is set to True\n",
      "Extracting Observations for ObservedProperty PM10\n"
     ]
    }
   ],
   "source": [
    "#input values. Read config.txt\n",
    "\n",
    "#set up which stations to parse here\n",
    "listtoparse=[]\n",
    "#listtoparse.append('DEBY006') #DEBY006 Augsburg Königsplatz\n",
    "#listtoparse.append('DEBY007') #DEBY007 Augsburg Bourges Platz\n",
    "#listtoparse.append('DEBY008') #DEBY008 Augsburg Haunstetten\n",
    "#listtoparse.append('DEBY081') #DEBY081 Garmisch Kreuzeckbahnstr\n",
    "#listtoparse.append('DEBY082') #DEBY082 Garmisch Wankgipfel\n",
    "#listtoparse.append('DEBY083') #DEBY083 Garmisch Zugspitzgipfel - no information about sensors\n",
    "#listtoparse.append('DEBY099') #DEBY099 Augsburg LfU\n",
    "#listtoparse.append('DEBY110') #DEBY110 Augsburg Karlstraße\n",
    "#listtoparse.append('DEBY123') #DEBY123 Garmisch am Herrgottschrofen\n",
    "#listtoparse.append('DEBY150') #DEBY150 Augsburg DBS - no information about sensors\n",
    "#listtoparse.append('DEBY196') #DEBY196 Garmisch Wasserwerk\n",
    "#add more to the list to parse\n",
    "\n",
    "\n",
    "try:\n",
    "    configfile=open('config.txt')\n",
    "    config=json.loads(configfile.read())\n",
    "\n",
    "    url = config['url']\n",
    "    listtoparse.append(config['thingcode'])\n",
    "    feature = config['feature']\n",
    "#    intervalllength = config['intervalllength']\n",
    "except:\n",
    "    sys.exit(\"Config File not properly set!\")\n",
    "\n",
    "\n",
    "upload=True #set to True/False to enable/disable upload of metadata to FROST\n",
    "\n",
    "\n",
    "#url = 'http://smartaqnet-dev.teco.edu:8080/FROST-Server/v1.0'\n",
    "#feature='PM10'\n",
    "\n",
    "\n",
    "\n",
    "baseurl = 'https://www.umweltbundesamt.de/js/uaq/data/stations' #get data where from\n",
    "scope='1SMW' #umweltbundesamt website code: 1 hour means\n",
    "scopesec= 60*60 #scope in seconds, needed for interval to tag the next observation\n",
    "\n",
    "print(\"Upload is set to \" + str(upload))\n",
    "print(\"Extracting Observations for ObservedProperty \" + str(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to check which stations contain which observedproperty\n",
    "#fulllist=[]\n",
    "#for ind in range(len(list(filemeta[\"component_code\"]))):\n",
    "#    if list(filemeta[\"component_code\"])[ind] == 5.0:\n",
    "#        fulllist.append([list(filemeta[\"station_code\"])[ind],list(filemeta[\"parameter\"])[ind]])\n",
    "#\n",
    "#fulllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: Going to uploading data for station DEBY007 at Augsburg/Bourges-Platz\n"
     ]
    }
   ],
   "source": [
    "#input parameters\n",
    "\n",
    "listofreplacements=[\n",
    "    ('PM10','5.0'),\n",
    "    ('PM2,5','6001.0'),\n",
    "    ('PM1','6002.0'),\n",
    "]\n",
    "\n",
    "#fetches the code for the above defined 'feature'\n",
    "for eachelement in listofreplacements:\n",
    "    if eachelement[0] == feature:\n",
    "        code = eachelement[1]\n",
    "\n",
    "\n",
    "file = pd.read_excel('metadata/Bericht_EU_Meta_Stationen.xlsx')\n",
    "filemeta=pd.read_excel('metadata/Bericht_EU_Meta_Stationsparameter.xlsx')\n",
    "df_stationparameters = filemeta.set_index(\"station_code\")\n",
    "\n",
    "#federal state, network code, website\n",
    "ccodesetreadable=[('Hessen', 'DE009A','hlnug.de'),\n",
    " ('Saarland', 'DE001A','saarland.de'),\n",
    " ('Berlin', 'DE008A','berlin.de'),\n",
    " ('Bayern', 'DE007A','lfu.bayern.de'),\n",
    " ('Rheinland-Pfalz', 'DE011A','luft.rlp.de'),\n",
    " ('Sachsen', 'DE016A','umwelt.sachsen.de'),\n",
    " ('Umweltbundesamt', 'DE006A','umweltbundesamt.de'),\n",
    " ('Baden-Wuerttemberg', 'DE005A','lubw.baden-wuerttemberg.de'),\n",
    " ('Nordrhein-Westfalen', 'DE004A','lanuv.nrw.de'),\n",
    " ('Brandenburg', 'DE014A','lfu.brandenburg.de'),\n",
    " ('Bremen', 'DE013A','bauumwelt.bremen.de'),\n",
    " ('Mecklenburg-Vorpommern', 'DE018A','lung.mv-regierung.de'),\n",
    " ('Hamburg', 'DE012A','luft.hamburg.de'),\n",
    " ('Thueringen', 'DE017A','tlug-jena.de'),\n",
    " ('Schleswig-Holstein', 'DE002A','schleswig-holstein.de'),\n",
    " ('Sachsen-Anhalt', 'DE015A','luesa.sachsen-anhalt.de'),\n",
    " ('Niedersachsen', 'DE010A','umwelt.niedersachsen.de')]\n",
    "\n",
    "\n",
    "#crosscheck\n",
    "for element in listtoparse:\n",
    "    print('Check: Going to uploading data for station ' + str(element) + ' at ' + str(file.set_index(\"station_code\")[\"station_name\"][element]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the url for the network code\n",
    "def repnetcodebyurl(netcode):\n",
    "    for item in ccodesetreadable:\n",
    "        if item[1]==netcode:\n",
    "            return(item[2])\n",
    "\n",
    "#returns the name of the state for the network code\n",
    "def repnetcodebystate(netcode):\n",
    "    for item in ccodesetreadable:\n",
    "        if item[1]==netcode:\n",
    "            return(item[0])\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#functions for time conversion\n",
    "def readtime(sec):\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return(str(int(h)) + \" hours \" + str(int(m)) + \" minutes \" + str(int(s)) + \" seconds\")\n",
    "\n",
    "def tounixtime(datetime_input):\n",
    "    return(calendar.timegm(datetime_input.utctimetuple()))\n",
    "\n",
    "def todatetimeformat(utctime):\n",
    "    year=int(utctime[0])*1000 + int(utctime[1])*100 + int(utctime[2])*10 + int(utctime[3])\n",
    "    month=int(utctime[5])*10 + int(utctime[6])\n",
    "    day=int(utctime[8])*10 + int(utctime[9])\n",
    "    hr=int(utctime[11])*10 + int(utctime[12])\n",
    "    minute=int(utctime[14])*10 + int(utctime[15])\n",
    "    second=int(utctime[17])*10 + int(utctime[18])\n",
    "    millisecond=int(utctime[20])*100 + int(utctime[21])*10 + int(utctime[22])   \n",
    "    return(datetime(year,month,day,hr,minute,second,millisecond))\n",
    "\n",
    "def toutcformat(datetime_input):\n",
    "    tstr=str(datetime_input)\n",
    "    year=tstr[0]+tstr[1]+tstr[2]+tstr[3]\n",
    "    month=tstr[5]+tstr[6]\n",
    "    day=tstr[8]+tstr[9]\n",
    "    \n",
    "    try:\n",
    "        if type(int(tstr[11]+tstr[12]))==int:\n",
    "            hour=str(tstr[11]+tstr[12])\n",
    "    except:\n",
    "        hour='00'             #no hours given       \n",
    "    \n",
    "    try:\n",
    "        if type(int(tstr[14]+tstr[15]))==int:\n",
    "            minute=str(tstr[14]+tstr[15])\n",
    "    except:\n",
    "        minute='00'             #no minutes given\n",
    "\n",
    "    try:\n",
    "        if type(int(tstr[17]+tstr[18]))==int:\n",
    "            second=str(tstr[17]+tstr[18])\n",
    "    except:\n",
    "        second='00'             #no seconds given\n",
    "        \n",
    "    try:\n",
    "        if type(int(tstr[20]+tstr[21]+tstr[22]))==int:\n",
    "            millisecond=str(tstr[20]+tstr[21]+tstr[22])\n",
    "    except:\n",
    "        millisecond='000'       #no milliseconds given\n",
    "\n",
    "\n",
    "        \n",
    "    utctime=year + '-' + month + '-' + day + 'T' + hour + ':' + minute + ':' + second + '.' + millisecond + 'Z'\n",
    "    return utctime\n",
    "\n",
    "def addminutesutc(utctime,mins):\n",
    "    return(toutcformat(datetime.utcfromtimestamp(tounixtime(todatetimeformat(utctime))+(mins*60))))\n",
    "\n",
    "currentyear=str(datetime.utcnow())[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Datastream saqn:d:lfu.bayern.de:unknown_type_nephelometry_and_beta_attenuation_sensor:comm201412:deby007:pm10\n",
      "Parsing start time not properly set. Taking earliest date measurements appear: 2016-12-31 00:00:00\n",
      "Measurements of DEBY007 Particulate matter - PM10, first measurement are being parsed from 2016-12-31 00:00:00\n",
      "Crosscheck for DEBY007 Particulate matter - PM10, first measurement with 10 random datapoints gave result True\n",
      "Time elapsed: 0 hours 0 minutes 43 seconds\n",
      "Loading file data\\DEBY007_Particulate matter - PM10, first measurement_2016-12-31_2018-12-31.xlsx...\n",
      "Uploading Observations for Datastream iot.id: saqn:d:lfu.bayern.de:unknown_type_nephelometry_and_beta_attenuation_sensor:comm201412:deby007:pm10\n",
      "Uploaded 14026 out of 17544 Observations. Estimating 0 hours 0 minutes 27 seconds remaining \r"
     ]
    }
   ],
   "source": [
    "#metadata\n",
    "totalstarttime = time.time() #to check how long the upload took\n",
    "\n",
    "#observed property\n",
    "generatepropertyid= \"saqn:o:\" + str(feature).lower().replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "obsproperty = {\n",
    "    \"name\": str(feature),\n",
    "    \"description\": \"\",\n",
    "    \"definition\": \"\",\n",
    "    \"@iot.id\": str(generatepropertyid).lower().replace(\" \", \"_\")\n",
    "    }\n",
    "if upload==True:\n",
    "    requests.post(url + '/ObservedProperties', json.dumps(obsproperty))\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#list over all stations in the given list\n",
    "\n",
    "for thingcode in listtoparse:\n",
    "    \n",
    "    thingnr=list(file[\"station_code\"]).index(thingcode) #the number of the row in the excel file\n",
    "    \n",
    "    generatedescr=\"\" #generates the description for the thing\n",
    "    if  df_stationparameters[\"type_of_parameter\"].index.contains(thingcode): #checks whether the station actually exists\n",
    "        if thingcode in df_stationparameters[\"type_of_parameter\"][thingcode]: #if the stations measures only one type of parameter, the index is not returned somehow and the loop produces an error, therefore check if the index is returned and if not handle case separately\n",
    "            for element in list(set(df_stationparameters[\"type_of_parameter\"][thingcode])):\n",
    "                generatedescr+= \" -\" + element + \"-\"\n",
    "        else: #if the station only measures one type of parameter (loop produces an error in that case, thus handled separately)\n",
    "            generatedescr+= \" -\" + df_stationparameters[\"type_of_parameter\"][thingcode] + \"-\"\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    #building things\n",
    "    \n",
    "    #Location ID\n",
    "    generatelocid = \"geo:\" + str(float(file[\"station_longitude_d\"][thingnr])) + \",\" + str(float(file[\"station_latitude_d\"][thingnr])) + \",\" + str(float(file[\"station_altitude\"][thingnr]))\n",
    "    \n",
    "    #Thing ID\n",
    "    generatethingid=\"saqn:t\" #generates the id for the thing by adding each of the following features to define it uniquely\n",
    "    generatethingid+=\":\" + str(repnetcodebyurl(file[\"network_code\"][thingnr])) #adds the url through the network code\n",
    "    generatethingid+=\":\" + str(file[\"station_name\"][thingnr])\n",
    "    generatethingid+=\":\" + \"comm\" + str(file[\"station_start_date\"][thingnr])[0:6]\n",
    "    generatethingid+=\":\" + str(file[\"station_code\"][thingnr])\n",
    "#    generatethingid+=\":\" + str(currentyear) #adds the current year to make the identifier unique in case the id gets changed one day\n",
    "#    for label in [\"station_name\",\"station_start_date\",\"station_code\"]: #add more info if desired\n",
    "#        generatethingid+=\":\" + str(file[label][thingnr])\n",
    "\n",
    "    #generates a dictionary of all raw properties of the thing\n",
    "    rawproperties = {}\n",
    "    for eachproperty in list(file):\n",
    "        if str(file[eachproperty][thingnr])=='nan':\n",
    "            rawproperties[eachproperty]='nan'\n",
    "        else:\n",
    "            rawproperties[eachproperty] = file[eachproperty][thingnr]\n",
    "\n",
    "    #generate the thing JSON\n",
    "    thingdata = {\"name\": \"Measuring Station \" + str(thingcode),\n",
    "        \"description\": \"A station measuring\" + str(generatedescr),\n",
    "        \"properties\": rawproperties,\n",
    "        \"@iot.id\": str(generatethingid).lower().replace(\" \", \"_\").replace(\"/\", \"_\"),\n",
    "         \"Locations\": [{\n",
    "            \"name\": \"Location of \" + \"measuring Station \" + str(thingcode),\n",
    "            \"description\": \"located at \" + str(file[\"station_name\"][thingnr]),\n",
    "            \"encodingType\": \"application/vnd.geo+json\",\n",
    "            \"@iot.id\": str(generatelocid),\n",
    "            \"location\": {\n",
    "                  \"type\": \"Point\",\n",
    "                  \"coordinates\": [float(file[\"station_latitude_d\"][thingnr]), float(file[\"station_longitude_d\"][thingnr]), float(file[\"station_altitude\"][thingnr])]\n",
    "            }\n",
    "             \n",
    "          }]\n",
    "    }\n",
    "    if upload==True:\n",
    "        requests.post(url + '/Things', json.dumps(thingdata, cls=MyEncoder))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "    #loop over all sensors and check which contains the requested observedproperty\n",
    "    try:\n",
    "        if float(code) not in list(df_stationparameters[\"component_code\"][thingcode]):\n",
    "            print(\"Station \" + str(thingcode) + \" does not measure \" + str(feature))\n",
    "        for sensnr in range(len(list(df_stationparameters[\"component_code\"][thingcode]))):\n",
    "            if list(df_stationparameters[\"component_code\"][thingcode])[sensnr] == float(code):\n",
    "                thissensor=list(df_stationparameters[\"parameter\"][thingcode])[sensnr] #the parameter to parse, e.g. \"Particulate Matter - PM10, first measurement\"\n",
    "                mestech=list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]\n",
    "                #warning: if only one sensor exists, this will blow up because one item is not returned as dataframe but as string\n",
    "\n",
    "                #------------------------------------------------------------------------------------------------\n",
    "                #building the sensors\n",
    "                #generates a dictionary of all raw properties of the thing to dump into metadata property\n",
    "                rawmetadata = {}\n",
    "                rawmetadata[\"station_code\"]=thingcode\n",
    "                for eachdata in list(df_stationparameters): #option 1: all metadata\n",
    "                #for eachdata in [\"type_of_parameter\",\"parameter\",\"component_code\",\"measurement_technique_principle\"]: #option 2: pick\n",
    "                    if str(list(df_stationparameters[eachdata][thingcode])[sensnr]) == 'nan':\n",
    "                        rawmetadata[eachdata]='nan'\n",
    "                    else:\n",
    "                        rawmetadata[eachdata] = list(df_stationparameters[eachdata][thingcode])[sensnr]\n",
    "\n",
    "\n",
    "                #Sensor ID - example: saqn:s:lfu.bayern.de:particulate_matter_-_pm10_first_measurement:nephelometry_and_beta_attenuation:2019:deby007\n",
    "\n",
    "                generatesensorid=\"saqn:s\" #generates the id for the sensor by adding each of the following features to define it uniquely\n",
    "                generatesensorid+=\":\" + str(repnetcodebyurl(file[\"network_code\"][thingnr])) #adds the url through the network code\n",
    "                generatesensorid+=\":\" + \"unknown_type_\" + str(list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]) + \"_sensor\"\n",
    "\n",
    "\n",
    "    #                for label in [\"parameter\",\"measurement_technique_principle\"]: #add more info if desired\n",
    "    #                    generatesensorid+=\":\" + str(list(df_stationparameters[label][thingcode])[sensnr])\n",
    "    #                generatesensorid+=\":\" + str(currentyear) #adds the current year to make the identifier unique in case the id gets changed one day\n",
    "    #                generatesensorid+=\":\" + str(thingcode) #code of the corresponding station\n",
    "\n",
    "                #generate sensor JSON\n",
    "                sensor = {\"name\": \"A \" + str(feature) + \" sensor\",\n",
    "                        \"description\": \"A sensor measuring \" + str(feature) + \" using \" + str(mestech),\n",
    "                        \"encodingType\": \"application/json\",\n",
    "                        \"metadata\": \"\",\n",
    "                        \"@iot.id\": str(generatesensorid).lower().replace(\" \", \"_\")\n",
    "                        }\n",
    "                if upload==True:\n",
    "                    requests.post(url + '/Sensors', json.dumps(sensor, cls=MyEncoder))\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                #------------------------------------------------------------------------------------------------\n",
    "                #building the datastreams\n",
    "\n",
    "                generatestreamid = \"saqn:d\"\n",
    "                generatestreamid+=\":\" + str(repnetcodebyurl(file[\"network_code\"][thingnr]))\n",
    "                generatestreamid+=\":\" + \"unknown_type_\" + str(list(df_stationparameters[\"measurement_technique_principle\"][thingcode])[sensnr]) + \"_sensor\"\n",
    "                generatestreamid+=\":\" + \"comm\" + str(list(df_stationparameters[\"measurement_start_date\"][thingcode])[sensnr])[0:6]\n",
    "                generatestreamid+=\":\" + str(thingcode)\n",
    "                generatestreamid+=\":\" + str(feature)\n",
    "\n",
    "                print(\"Building Datastream \" + str(generatestreamid).lower().replace(\" \", \"_\"))\n",
    "\n",
    "                datastream = {\"name\": str(thissensor) + \" Datastream of station \" + str(thingcode),\n",
    "                            \"description\": \"A Datastream measuring \" + str(thissensor) + \" using \" + str(mestech),\n",
    "                            \"observationType\": \"\",\n",
    "                            \"unitOfMeasurement\": {\n",
    "                                \"name\": \"microgram per cubic meter\",\n",
    "                                \"symbol\": \"ug/m^3\",\n",
    "                                \"definition\": \"none\"\n",
    "                                },\n",
    "                            \"properties\": rawmetadata,\n",
    "                            \"@iot.id\": str(generatestreamid).lower().replace(\" \", \"_\"),\n",
    "                            \"Thing\":{\"@iot.id\":str(generatethingid).lower().replace(\" \", \"_\").replace(\"/\", \"_\")},\n",
    "                            \"Sensor\":{\"@iot.id\":str(generatesensorid).lower().replace(\" \", \"_\")},\n",
    "                            \"ObservedProperty\":{\"@iot.id\":str(generatepropertyid).lower().replace(\" \", \"_\")}\n",
    "                            }\n",
    "\n",
    "                if upload==True:\n",
    "                    requests.post(url + '/Datastreams', json.dumps(datastream))\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                #END OF METADATA - - - BEGIN OF DATA UPLOAD\n",
    "                #------------------------------------------------------------\n",
    "\n",
    "                def graburl(start,end): #start and end in datetime\n",
    "                    return(baseurl + '/measuring?pollutant[]=' + feature + '&scope[]=' + scope + '&station[]=' + thingcode + '&group[]=pollutant&range[]=' + str(tounixtime(start)) + ',' + str(tounixtime(end)))\n",
    "\n",
    "                #begin time\n",
    "                try:\n",
    "                    begintime=todatetimeformat(config['starttime'])\n",
    "                except:\n",
    "\n",
    "                    #get the start month of the respective datastream\n",
    "\n",
    "                    def getstarttime():\n",
    "                        for i in range(1970,2018+1): #ranges to 2018, the +1 is because how range counts\n",
    "                            try:\n",
    "                                theurl=graburl(datetime(i,1,1,0,0,0),datetime(i,12,31,23,59,0))\n",
    "                                datafromurl=json.loads(requests.get(theurl).content)[\"data\"][0]\n",
    "                                if datafromurl[0][0]!='bananas': #anything but an error\n",
    "                                    for j in range(1,12+1):\n",
    "                                        try:\n",
    "                                            theurl2=graburl(datetime(i,j,1,0,0,0),datetime(i,j,calendar.monthrange(i,j)[1],23,59,0))\n",
    "                                            datafromurl2=json.loads(requests.get(theurl2).content)[\"data\"][0]\n",
    "                                            if datafromurl2[0][0]!='bananas':\n",
    "                                                for k in range(1,calendar.monthrange(i,j)[1]+1):\n",
    "                                                    try:\n",
    "                                                        theurl3=graburl(datetime(i,j,k,0,0,0),datetime(i,j,k,23,59,0))\n",
    "                                                        datafromurl3=json.loads(requests.get(theurl3).content)[\"data\"][0]\n",
    "                                                        if datafromurl3[0][0]!='bananas':\n",
    "                                                            return(datetime(i,j,k,0,0,0))\n",
    "                                                            break\n",
    "                                                    except:\n",
    "                                                        pass\n",
    "                                        except:\n",
    "                                            pass\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                    begintime=getstarttime()\n",
    "                    print(\"Parsing start time not properly set. Taking earliest date measurements appear: \" + str(begintime))\n",
    "\n",
    "                #end time\n",
    "                try:\n",
    "                    endtime=todatetimeformat(config['endtime'])\n",
    "                except:\n",
    "                    print(\"Parsing end time not properly set. Taking \" + str(datetime.utcnow()))\n",
    "                    endtime  =datetime.utcnow() \n",
    "\n",
    "\n",
    "                #------------------------------------------------------------\n",
    "                print('Measurements of ' + str(thingcode) + ' ' +  str(thissensor) + ' are being parsed from ' + str(begintime))\n",
    "\n",
    "                begintimeunix=tounixtime(begintime)\n",
    "                endtimeunix=tounixtime(endtime)\n",
    "\n",
    "                getdatafrom=baseurl + '/measuring?pollutant[]=' + feature + '&scope[]=' + scope + '&station[]=' + thingcode + '&group[]=pollutant&range[]=' + str(begintimeunix + (scopesec/2)) + ',' + str(endtimeunix  + (scopesec/2))\n",
    "                datavalue=json.loads(requests.get(getdatafrom).content)[\"data\"][0]\n",
    "\n",
    "\n",
    "                #convert list into a dataframe\n",
    "                datalist=[]\n",
    "                labels=['interval_start_time','interval_end_time',str(feature)]\n",
    "\n",
    "                for i in range(len(datavalue)):\n",
    "                        datalist.append([toutcformat(datetime.utcfromtimestamp(begintimeunix + (scopesec*i))),toutcformat(datetime.utcfromtimestamp(begintimeunix + ((scopesec*i) + (scopesec-60)) )),datavalue[i][0]])\n",
    "\n",
    "                dataframe = pd.DataFrame.from_records(datalist, columns=labels)\n",
    "                dataframemeta = pd.DataFrame.from_records([[generatestreamid.lower().replace(\" \", \"_\")]], columns=['datastreamID'])\n",
    "\n",
    "                #save dataframe to excel sheet\n",
    "                filename= str(thingcode) + \"_\" + str(thissensor) + \"_\" + str(toutcformat(begintime)[0:10]) + \"_\" + str(toutcformat(endtime)[0:10]) + \".xlsx\"\n",
    "                writer = ExcelWriter('data/' +  str(filename))\n",
    "                dataframe.to_excel(writer,'Sheet1',index=False)\n",
    "                dataframemeta.to_excel(writer,'id',index=False)\n",
    "                writer.save()\n",
    "\n",
    "                #crosschecking: checks a number of random points between the dataframe and the url\n",
    "\n",
    "                #the UBA database gives the result for the hour that has PASSED, e.g. the value at 5 o'clock is the mean between 4 and 5 o'clock\n",
    "                #the code writes a value for the interval between e.g. 3:00 and 3:59. for this interval, the UBA value at 3:30 is taken. \n",
    "                #therefore the check also needs to add 30m to the url to check with the corresponding start time\n",
    "\n",
    "                numberoftests=10\n",
    "                testistrue=False\n",
    "\n",
    "\n",
    "                for i in range(0,10):\n",
    "                    r = random.randint(1,len(list(dataframe[\"interval_start_time\"])))\n",
    "                    getdatapoint=graburl(todatetimeformat(addminutesutc(dataframe[\"interval_start_time\"][r],30)),todatetimeformat(addminutesutc(dataframe[\"interval_end_time\"][r],30)))\n",
    "                    checkdatavalue=json.loads(requests.get(getdatapoint).content)[\"data\"][0][0][0]\n",
    "                    if dataframe[feature][r] == checkdatavalue:\n",
    "                        testistrue=True\n",
    "                    else:\n",
    "                        print(str(dataframe[feature][r]) + \"!=\" + str(checkdatavalue))\n",
    "                        testistrue=False\n",
    "\n",
    "                print(\"Crosscheck for \" + str(thingcode) + \" \" + str(thissensor) + \" with \" + str(numberoftests) + \" random datapoints gave result \" + str(testistrue))\n",
    "    except:\n",
    "        print(\"Could not retrieve any data for station \" + str(thingcode))\n",
    "    \n",
    "    \n",
    "endtime=time.time()\n",
    "timeelapsed=endtime-totalstarttime\n",
    "    \n",
    "print(\"Time elapsed: \" + str(readtime(timeelapsed)))\n",
    "\n",
    "if config['runparseaftermodelling']==True:\n",
    "    exec(open('UBA_parse_excel.py').read())\n",
    "    Print['Feeding Measurements into ' + str(url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
